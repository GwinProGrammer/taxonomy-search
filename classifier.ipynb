{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"  # or \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"gov_docs.csv\")\n",
    "df[\"label\"] = df[\"category\"] + \"|\" + df[\"subcategory\"]  # e.g., \"healthcare|medicaid\"\n",
    "\n",
    "# Map labels to IDs\n",
    "labels = df[\"label\"].unique().tolist()\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for i, l in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    category subcategory  \\\n",
      "0  FDADrug Safety Communication FDA cautions agai...  healthcare         FDA   \n",
      "1  Field Alert Report Submission Questions and An...  healthcare         FDA   \n",
      "2  Acceptability of Draft Labeling to Support AND...  healthcare         FDA   \n",
      "3  Contains Nonbinding Recommendations The Least ...  healthcare         FDA   \n",
      "4  Center for Devices and Radiological Health Int...  healthcare         FDA   \n",
      "\n",
      "                                         source_file           label  \n",
      "0     Updated.DSC-Hydroxychloroquine.chloroquine.txt  healthcare|FDA  \n",
      "1                                   24740676_FAR.txt  healthcare|FDA  \n",
      "2  Acceptability-of-Draft-Labeling-to-Support-Abb...  healthcare|FDA  \n",
      "3                                           1332.txt  healthcare|FDA  \n",
      "4  CDRH_International_Harmonization_Draft_Strateg...  healthcare|FDA  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text column (run this before tokenization)\n",
    "df['text'] = df['text'].fillna('')  # Replace NaN\n",
    "df['text'] = df['text'].astype(str)  # Force string type\n",
    "df['text'] = df['text'].str.strip()  # Remove whitespace\n",
    "\n",
    "# Remove empty texts if needed\n",
    "df = df[df['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 207, Validation samples: 52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "print(f\"Train samples: {len(train_df)}, Validation samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing test text: FDADrug Safety Communication FDA cautions against  ...\n",
      "Success! Tokenized output keys: dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Test tokenization on first row\n",
    "try:\n",
    "    test_text = df.iloc[0]['text']\n",
    "    print(\"\\nTokenizing test text:\", test_text[:50], \"...\")\n",
    "    tokens = tokenizer(test_text, truncation=True)\n",
    "    print(\"Success! Tokenized output keys:\", tokens.keys())\n",
    "except Exception as e:\n",
    "    print(f\"Failed: {type(e).__name__}: {e}\")\n",
    "    print(\"Problem text:\", test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13afb179068d41358b2a050d60078a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/259 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491363015be9467a8827b4a1bbea4c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690e04b840f94f4c97569d7fb459ea25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  # Automatically uses fast tokenizer\n",
    "\n",
    "# Tokenize in batches (CPU-friendly)\n",
    "def tokenize(batch):\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"np\"  # Keep NumPy arrays for CPU efficiency\n",
    "    )\n",
    "    \n",
    "    # Convert text labels to numerical IDs using your label2id mapping\n",
    "    tokenized[\"labels\"] = np.array([label2id[label] for label in batch[\"label\"]])\n",
    "    \n",
    "    return tokenized\n",
    "# def tokenize(batch):\n",
    "#     return tokenizer(\n",
    "#         batch[\"text\"],\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",  # Pad to max_length for static shapes (better CPU performance)\n",
    "#         max_length=512,        # DistilBERT's limit\n",
    "#         return_tensors=\"np\"    # NumPy arrays for CPU (smaller memory footprint)\n",
    "#     )\n",
    "\n",
    "# Apply to datasets\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(tokenize, batched=True, batch_size=100)  # Process 100 texts at once\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df).map(tokenize, batched=True, batch_size=100)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(tokenize, batched=True, batch_size=100)\n",
    "\n",
    "columns_to_remove = ['text', 'category', 'subcategory', 'source_file', 'label', '__index_level_0__']\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "val_dataset = val_dataset.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      " label\n",
      "healthcare|FDA                     30\n",
      "healthcare|medicaid                30\n",
      "healthcare|medicare                28\n",
      "education|k12funding               28\n",
      "defense|cybersecurity              28\n",
      "defense|procurement                26\n",
      "education|studentloans             24\n",
      "finance|budgets                    24\n",
      "education|highereducationpolicy    23\n",
      "finance|tax_policies               18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = df['label'].value_counts()  # Count occurrences of each label\n",
    "print(\"Label distribution:\\n\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training labels: 9\n",
      "Label mapping: {'healthcare|FDA': 0, 'healthcare|medicaid': 1, 'healthcare|medicare': 2, 'education|highereducationpolicy': 3, 'education|k12funding': 4, 'education|studentloans': 5, 'finance|tax_policies': 6, 'finance|budgets': 7, 'defense|cybersecurity': 8, 'defense|procurement': 9}\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample training labels:\", train_dataset[0][\"labels\"])  # Should be an integer\n",
    "print(\"Label mapping:\", label2id)  # Verify your mapping is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training sample: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "First validation sample: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# Verify all labels exist in your mapping\n",
    "missing_labels = set(df['label']) - set(label2id.keys())\n",
    "if missing_labels:\n",
    "    raise ValueError(f\"Labels missing from mapping: {missing_labels}\")\n",
    "\n",
    "# Check tokenized datasets\n",
    "print(\"First training sample:\", train_dataset[0].keys())\n",
    "print(\"First validation sample:\", val_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading CSV\n",
    "df = df.dropna(subset=['text', 'label'])  # Remove rows with missing text or label\n",
    "df = df[df['text'].str.strip() != '']     # Remove empty texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer\n",
    "import torch\n",
    "\n",
    "# Get class weights (critical for imbalanced data)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=df['label'].unique(),\n",
    "    y=df['label']\n",
    ")\n",
    "weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# Model with weighted loss\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"distilbert-base-uncased\",\n",
    "#     num_labels=len(label_counts),\n",
    "#     id2label={i: l for i, l in enumerate(label_counts.index)},\n",
    "#     label2id={l: i for i, l in enumerate(label_counts.index)}\n",
    "# )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(label2id),  # Use label2id count\n",
    "    id2label=id2label,         # Use your original mapping\n",
    "    label2id=label2id          # Use your original mapping\n",
    ")\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         loss_fct = torch.nn.CrossEntropyLoss(weight=weights.to(model.device))\n",
    "#         loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Ensure weights are on correct device\n",
    "        current_weights = weights.to(model.device)\n",
    "        \n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=current_weights)\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), \n",
    "                      labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwin/opt/anaconda3/envs/transformers/lib/python3.10/site-packages/transformers/training_args.py:1595: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./cpu_results\",\n",
    "    per_device_train_batch_size=4,  # Reduce batch size for CPU (typical: 4-8)\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,   # Simulate larger batches by accumulating gradients\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,                 # Check validation less frequently\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,             # Fewer epochs if CPU is slow\n",
    "    fp16=False,                  # Disable mixed-precision (CPU doesn't support it)\n",
    "    no_cuda=True,                   # Ensure no GPU is accidentally used\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Train 'labels' type: <class 'int'>\n",
      "Validation sample keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Validation 'labels' type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Check the first sample in train_dataset\n",
    "print(\"Train sample keys:\", train_dataset[0].keys())\n",
    "print(\"Train 'labels' type:\", type(train_dataset[0][\"labels\"]))  # Should be `int` or `numpy.int64`\n",
    "\n",
    "# Check the first sample in val_dataset\n",
    "print(\"Validation sample keys:\", val_dataset[0].keys())\n",
    "print(\"Validation 'labels' type:\", type(val_dataset[0][\"labels\"]))  # Should be `int` or `numpy.int64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 03:27, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.2968854904174805, metrics={'train_runtime': 228.6527, 'train_samples_per_second': 0.35, 'train_steps_per_second': 0.044, 'total_flos': 10598903808000.0, 'train_loss': 2.2968854904174805, 'epoch': 0.38461538461538464})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use tokenized training data\n",
    "    eval_dataset=val_dataset,     # Use tokenized validation data\n",
    ")\n",
    "\n",
    "trainer.train()  # Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training (BEFORE kernel restart)\n",
    "model.save_pretrained(\"./my_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./my_finetuned_model\")\n",
    "\n",
    "# Save label mappings\n",
    "import json\n",
    "with open(\"./my_finetuned_model/label_mappings.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"label2id\": label2id,  # Your existing mapping dict\n",
    "        \"id2label\": id2label   # Your existing mapping dict\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: education|k12funding\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_path = \"./my_finetuned_model\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example: Classify new text\n",
    "text = \"Medicaid budget 2024\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "pred_label_id = np.argmax(outputs.logits.detach().numpy())\n",
    "pred_label = id2label[pred_label_id]  # Use your `id2label` mapping\n",
    "print(f\"Predicted: {pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                 healthcare|FDA       0.14      0.17      0.15         6\n",
      "            healthcare|medicaid       0.20      0.17      0.18         6\n",
      "            healthcare|medicare       0.00      0.00      0.00         6\n",
      "education|highereducationpolicy       0.00      0.00      0.00         4\n",
      "           education|k12funding       0.12      0.83      0.22         6\n",
      "         education|studentloans       0.00      0.00      0.00         5\n",
      "           finance|tax_policies       0.00      0.00      0.00         3\n",
      "                finance|budgets       0.00      0.00      0.00         5\n",
      "          defense|cybersecurity       0.00      0.00      0.00         6\n",
      "            defense|procurement       0.00      0.00      0.00         5\n",
      "\n",
      "                       accuracy                           0.13        52\n",
      "                      macro avg       0.05      0.12      0.06        52\n",
      "                   weighted avg       0.05      0.13      0.06        52\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwin/opt/anaconda3/envs/transformers/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/gwin/opt/anaconda3/envs/transformers/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/gwin/opt/anaconda3/envs/transformers/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "predictions = trainer.predict(val_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions.label_ids, preds, target_names=list(label2id.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample labels before conversion:\n",
      "267      defense|procurement\n",
      "247      defense|procurement\n",
      "245      defense|procurement\n",
      "227    defense|cybersecurity\n",
      "12            healthcare|FDA\n",
      "Name: label, dtype: object\n",
      "\n",
      "Label to ID mapping:\n",
      "{'healthcare|FDA': 0, 'healthcare|medicaid': 1, 'healthcare|medicare': 2, 'education|highereducationpolicy': 3, 'education|k12funding': 4, 'education|studentloans': 5, 'finance|tax_policies': 6, 'finance|budgets': 7, 'defense|cybersecurity': 8, 'defense|procurement': 9}\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample labels before conversion:\")\n",
    "print(train_df['label'].head())\n",
    "\n",
    "print(\"\\nLabel to ID mapping:\")\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 3189, 2053, 1012, 26489, 8004, 1514, 2355, 1514, 5511, 2629, 1057, 1012, 1055, 1012, 2533, 1997, 3639, 1045, 1050, 1055, 1052, 1041, 1039, 2000, 1054, 1043, 1041, 1050, 1041, 1054, 1037, 1048, 2258, 2756, 1010, 2355, 1996, 2250, 2486, 6194, 2005, 10439, 22046, 2250, 2486, 2166, 5402, 2968, 2415, 2309, 1514, 2400, 25617, 1514, 6959, 25617, 1514, 11712, 8311, 2342, 7620, 11109, 8122, 17842, 8012, 18447, 13910, 15780, 8122, 17842, 8012, 25481, 2256, 3260, 2003, 2000, 3073, 2981, 1010, 7882, 1010, 1998, 23259, 15709, 1997, 1996, 2533, 1997, 3639, 2008, 6753, 1996, 2162, 20027, 1025, 14067, 17842, 1010, 11109, 1010, 1998, 8122, 1025, 25453, 1996, 3187, 1997, 3639, 1998, 3519, 1025, 1998, 15670, 1996, 2270, 1012, 4432, 2256, 4432, 2003, 2000, 2022, 1037, 2944, 15709, 3029, 1999, 1996, 2976, 2231, 2011, 2877, 2689, 1010, 4092, 3606, 1010, 1998, 7694, 8012, 1517, 1037, 7578, 3029, 1010, 2551, 2362, 2004, 2028, 2658, 2136, 1010, 3858, 2004, 4177, 1999, 2256, 2492, 1012, 9861, 1010, 5949, 1004, 6905, 2980, 18194, 13699, 8445, 3672, 1997, 3639, 26489, 8004, 1012, 23689, 1013, 2980, 4179, 1064, 5385, 1012, 4413, 2549, 1012, 3938, 2683, 2620, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 9}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])  # Should ONLY show: input_ids, attention_mask, labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
