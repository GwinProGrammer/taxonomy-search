{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"  # or \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"gov_docs.csv\")\n",
    "df[\"label\"] = df[\"category\"] + \"|\" + df[\"subcategory\"]  # e.g., \"healthcare|medicaid\"\n",
    "\n",
    "# Map labels to IDs\n",
    "labels = df[\"label\"].unique().tolist()\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for i, l in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    category subcategory  \\\n",
      "0  FDADrug Safety Communication FDA cautions agai...  healthcare         FDA   \n",
      "1  Field Alert Report Submission Questions and An...  healthcare         FDA   \n",
      "2  Acceptability of Draft Labeling to Support AND...  healthcare         FDA   \n",
      "3  Contains Nonbinding Recommendations The Least ...  healthcare         FDA   \n",
      "4  Center for Devices and Radiological Health Int...  healthcare         FDA   \n",
      "\n",
      "                                         source_file           label  \n",
      "0     Updated.DSC-Hydroxychloroquine.chloroquine.txt  healthcare|FDA  \n",
      "1                                   24740676_FAR.txt  healthcare|FDA  \n",
      "2  Acceptability-of-Draft-Labeling-to-Support-Abb...  healthcare|FDA  \n",
      "3                                           1332.txt  healthcare|FDA  \n",
      "4  CDRH_International_Harmonization_Draft_Strateg...  healthcare|FDA  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text column (run this before tokenization)\n",
    "df['text'] = df['text'].fillna('')  # Replace NaN\n",
    "df['text'] = df['text'].astype(str)  # Force string type\n",
    "df['text'] = df['text'].str.strip()  # Remove whitespace\n",
    "\n",
    "# Remove empty texts if needed\n",
    "df = df[df['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 207, Validation samples: 52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "print(f\"Train samples: {len(train_df)}, Validation samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing test text: FDADrug Safety Communication FDA cautions against  ...\n",
      "Success! Tokenized output keys: dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Test tokenization on first row\n",
    "try:\n",
    "    test_text = df.iloc[0]['text']\n",
    "    print(\"\\nTokenizing test text:\", test_text[:50], \"...\")\n",
    "    tokens = tokenizer(test_text, truncation=True)\n",
    "    print(\"Success! Tokenized output keys:\", tokens.keys())\n",
    "except Exception as e:\n",
    "    print(f\"Failed: {type(e).__name__}: {e}\")\n",
    "    print(\"Problem text:\", test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c7e2c5f0854af29989e2088ba5eaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/259 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  # Automatically uses fast tokenizer\n",
    "\n",
    "# Tokenize in batches (CPU-friendly)\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # Pad to max_length for static shapes (better CPU performance)\n",
    "        max_length=512,        # DistilBERT's limit\n",
    "        return_tensors=\"np\"    # NumPy arrays for CPU (smaller memory footprint)\n",
    "    )\n",
    "\n",
    "# Apply to datasets\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(tokenize, batched=True, batch_size=100)  # Process 100 texts at once\n",
    "train_dataset = Dataset.from_pandas(train_df).map(tokenize, batched=True, batch_size=100)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(tokenize, batched=True, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      " label\n",
      "healthcare|FDA                     30\n",
      "healthcare|medicaid                30\n",
      "healthcare|medicare                28\n",
      "education|k12funding               28\n",
      "defense|cybersecurity              28\n",
      "defense|procurement                26\n",
      "education|studentloans             24\n",
      "finance|budgets                    24\n",
      "education|highereducationpolicy    23\n",
      "finance|tax_policies               18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# label_counts = df['label'].value_counts()  # Count occurrences of each label\n",
    "# print(\"Label distribution:\\n\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer\n",
    "import torch\n",
    "\n",
    "# Get class weights (critical for imbalanced data)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=df['label'].unique(),\n",
    "    y=df['label']\n",
    ")\n",
    "weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# Model with weighted loss\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(label_counts),\n",
    "    id2label={i: l for i, l in enumerate(label_counts.index)},\n",
    "    label2id={l: i for i, l in enumerate(label_counts.index)}\n",
    ")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./cpu_results\",\n",
    "    per_device_train_batch_size=4,  # Reduce batch size for CPU (typical: 4-8)\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,   # Simulate larger batches by accumulating gradients\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,                 # Check validation less frequently\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,             # Fewer epochs if CPU is slow\n",
    "    fp16=False,                  # Disable mixed-precision (CPU doesn't support it)\n",
    "    no_cuda=True,                   # Ensure no GPU is accidentally used\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m----> 4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_dataset\u001b[49m,  \u001b[38;5;66;03m# Use tokenized training data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset,     \u001b[38;5;66;03m# Use tokenized validation data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Start training!\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use tokenized training data\n",
    "    eval_dataset=val_dataset,     # Use tokenized validation data\n",
    ")\n",
    "\n",
    "trainer.train()  # Start training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
