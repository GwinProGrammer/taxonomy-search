{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"  # or \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"gov_docs.csv\")\n",
    "df[\"label\"] = df[\"category\"] + \"|\" + df[\"subcategory\"]  # e.g., \"healthcare|medicaid\"\n",
    "\n",
    "# Map labels to IDs\n",
    "labels = df[\"label\"].unique().tolist()\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for i, l in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    category subcategory  \\\n",
      "0  FDADrug Safety Communication FDA cautions agai...  healthcare         FDA   \n",
      "1  Field Alert Report Submission Questions and An...  healthcare         FDA   \n",
      "2  Acceptability of Draft Labeling to Support AND...  healthcare         FDA   \n",
      "3  Contains Nonbinding Recommendations The Least ...  healthcare         FDA   \n",
      "4  Center for Devices and Radiological Health Int...  healthcare         FDA   \n",
      "\n",
      "                                         source_file           label  \n",
      "0     Updated.DSC-Hydroxychloroquine.chloroquine.txt  healthcare|FDA  \n",
      "1                                   24740676_FAR.txt  healthcare|FDA  \n",
      "2  Acceptability-of-Draft-Labeling-to-Support-Abb...  healthcare|FDA  \n",
      "3                                           1332.txt  healthcare|FDA  \n",
      "4  CDRH_International_Harmonization_Draft_Strateg...  healthcare|FDA  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text column (run this before tokenization)\n",
    "df['text'] = df['text'].fillna('')  # Replace NaN\n",
    "df['text'] = df['text'].astype(str)  # Force string type\n",
    "df['text'] = df['text'].str.strip()  # Remove whitespace\n",
    "\n",
    "# Remove empty texts if needed\n",
    "df = df[df['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 207, Validation samples: 52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "print(f\"Train samples: {len(train_df)}, Validation samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing test text: FDADrug Safety Communication FDA cautions against  ...\n",
      "Success! Tokenized output keys: dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Test tokenization on first row\n",
    "try:\n",
    "    test_text = df.iloc[0]['text']\n",
    "    print(\"\\nTokenizing test text:\", test_text[:50], \"...\")\n",
    "    tokens = tokenizer(test_text, truncation=True)\n",
    "    print(\"Success! Tokenized output keys:\", tokens.keys())\n",
    "except Exception as e:\n",
    "    print(f\"Failed: {type(e).__name__}: {e}\")\n",
    "    print(\"Problem text:\", test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b990b3fc087248aa890dba8403eab048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/259 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf30071b33c48819a799d0132e20180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3ebe00ddfd4f58bfba98052926f606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  # Automatically uses fast tokenizer\n",
    "\n",
    "# Tokenize in batches (CPU-friendly)\n",
    "def tokenize(batch):\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"np\"  # Keep NumPy arrays for CPU efficiency\n",
    "    )\n",
    "    \n",
    "    # Convert text labels to numerical IDs using your label2id mapping\n",
    "    tokenized[\"labels\"] = np.array([label2id[label] for label in batch[\"label\"]])\n",
    "    \n",
    "    return tokenized\n",
    "# def tokenize(batch):\n",
    "#     return tokenizer(\n",
    "#         batch[\"text\"],\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",  # Pad to max_length for static shapes (better CPU performance)\n",
    "#         max_length=512,        # DistilBERT's limit\n",
    "#         return_tensors=\"np\"    # NumPy arrays for CPU (smaller memory footprint)\n",
    "#     )\n",
    "\n",
    "# Apply to datasets\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(tokenize, batched=True, batch_size=100)  # Process 100 texts at once\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df).map(tokenize, batched=True, batch_size=100)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(tokenize, batched=True, batch_size=100)\n",
    "\n",
    "columns_to_remove = ['text', 'category', 'subcategory', 'source_file', 'label', '__index_level_0__']\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "val_dataset = val_dataset.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      " label\n",
      "healthcare|FDA                     30\n",
      "healthcare|medicaid                30\n",
      "healthcare|medicare                28\n",
      "education|k12funding               28\n",
      "defense|cybersecurity              28\n",
      "defense|procurement                26\n",
      "education|studentloans             24\n",
      "finance|budgets                    24\n",
      "education|highereducationpolicy    23\n",
      "finance|tax_policies               18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = df['label'].value_counts()  # Count occurrences of each label\n",
    "print(\"Label distribution:\\n\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training labels: 9\n",
      "Label mapping: {'healthcare|FDA': 0, 'healthcare|medicaid': 1, 'healthcare|medicare': 2, 'education|highereducationpolicy': 3, 'education|k12funding': 4, 'education|studentloans': 5, 'finance|tax_policies': 6, 'finance|budgets': 7, 'defense|cybersecurity': 8, 'defense|procurement': 9}\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample training labels:\", train_dataset[0][\"labels\"])  # Should be an integer\n",
    "print(\"Label mapping:\", label2id)  # Verify your mapping is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training sample: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "First validation sample: dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# Verify all labels exist in your mapping\n",
    "missing_labels = set(df['label']) - set(label2id.keys())\n",
    "if missing_labels:\n",
    "    raise ValueError(f\"Labels missing from mapping: {missing_labels}\")\n",
    "\n",
    "# Check tokenized datasets\n",
    "print(\"First training sample:\", train_dataset[0].keys())\n",
    "print(\"First validation sample:\", val_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading CSV\n",
    "df = df.dropna(subset=['text', 'label'])  # Remove rows with missing text or label\n",
    "df = df[df['text'].str.strip() != '']     # Remove empty texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer\n",
    "import torch\n",
    "\n",
    "# Get class weights (critical for imbalanced data)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=df['label'].unique(),\n",
    "    y=df['label']\n",
    ")\n",
    "weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# Model with weighted loss\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"distilbert-base-uncased\",\n",
    "#     num_labels=len(label_counts),\n",
    "#     id2label={i: l for i, l in enumerate(label_counts.index)},\n",
    "#     label2id={l: i for i, l in enumerate(label_counts.index)}\n",
    "# )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(label2id),  # Use label2id count\n",
    "    id2label=id2label,         # Use your original mapping\n",
    "    label2id=label2id          # Use your original mapping\n",
    ")\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         loss_fct = torch.nn.CrossEntropyLoss(weight=weights.to(model.device))\n",
    "#         loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Ensure weights are on correct device\n",
    "        current_weights = weights.to(model.device)\n",
    "        \n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=current_weights)\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), \n",
    "                      labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./cpu_training_results\",\n",
    "    # Batch sizes\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # Logging/Eval/Save (Aligned)\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,            # Log metrics every 50 steps\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,              # Evaluate every 200 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,              # Must equal eval_steps or be a multiple\n",
    "    \n",
    "    # Training\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    max_steps=-1,\n",
    "    \n",
    "    # CPU-specific\n",
    "    fp16=False,\n",
    "    no_cuda=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    # Model selection\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Misc\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Train 'labels' type: <class 'int'>\n",
      "Validation sample keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Validation 'labels' type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Check the first sample in train_dataset\n",
    "print(\"Train sample keys:\", train_dataset[0].keys())\n",
    "print(\"Train 'labels' type:\", type(train_dataset[0][\"labels\"]))  # Should be `int` or `numpy.int64`\n",
    "\n",
    "# Check the first sample in val_dataset\n",
    "print(\"Validation sample keys:\", val_dataset[0].keys())\n",
    "print(\"Validation 'labels' type:\", type(val_dataset[0][\"labels\"]))  # Should be `int` or `numpy.int64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 52:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=130, training_loss=1.7449928577129656, metrics={'train_runtime': 3179.7836, 'train_samples_per_second': 0.325, 'train_steps_per_second': 0.041, 'total_flos': 137123318016000.0, 'train_loss': 1.7449928577129656, 'epoch': 5.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Use tokenized training data\n",
    "    eval_dataset=val_dataset,     # Use tokenized validation data\n",
    ")\n",
    "\n",
    "trainer.train()  # Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: models/model_20250411_122546\n"
     ]
    }
   ],
   "source": [
    "# # After training (BEFORE kernel restart)\n",
    "# model.save_pretrained(\"./my_finetuned_model\")\n",
    "# tokenizer.save_pretrained(\"./my_finetuned_model\")\n",
    "\n",
    "# # Save label mappings\n",
    "# import json\n",
    "# with open(\"./my_finetuned_model/label_mappings.json\", \"w\") as f:\n",
    "#     json.dump({\n",
    "#         \"label2id\": label2id,  # Your existing mapping dict\n",
    "#         \"id2label\": id2label   # Your existing mapping dict\n",
    "#     }, f)\n",
    "\n",
    "# import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate a unique folder name (e.g., \"model_20240515_153022\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir = f\"models/model_{timestamp}\"\n",
    "\n",
    "# Save model, tokenizer, and mappings\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "with open(f\"{model_dir}/label_mappings.json\", \"w\") as f:\n",
    "    json.dump({\"label2id\": label2id, \"id2label\": id2label}, f)\n",
    "\n",
    "print(f\"Model saved to: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: healthcare|medicaid\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# model_path = \"/Users/gwin/Documents/Post Undergrad Work/Tax Search/models/model_20250411_122546\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # Example: Classify new text\n",
    "# text = \"Medicaid budget 2024\"\n",
    "# inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "# outputs = model(**inputs)\n",
    "# pred_label_id = np.argmax(outputs.logits.detach().numpy())\n",
    "# pred_label = id2label[pred_label_id]  # Use your `id2label` mapping\n",
    "# print(f\"Predicted: {pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                 healthcare|FDA       0.86      1.00      0.92         6\n",
      "            healthcare|medicaid       1.00      0.83      0.91         6\n",
      "            healthcare|medicare       0.83      0.83      0.83         6\n",
      "education|highereducationpolicy       1.00      0.50      0.67         4\n",
      "           education|k12funding       1.00      1.00      1.00         6\n",
      "         education|studentloans       1.00      1.00      1.00         5\n",
      "           finance|tax_policies       1.00      1.00      1.00         3\n",
      "                finance|budgets       1.00      0.20      0.33         5\n",
      "          defense|cybersecurity       0.71      0.83      0.77         6\n",
      "            defense|procurement       0.50      1.00      0.67         5\n",
      "\n",
      "                       accuracy                           0.83        52\n",
      "                      macro avg       0.89      0.82      0.81        52\n",
      "                   weighted avg       0.88      0.83      0.81        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "predictions = trainer.predict(val_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions.label_ids, preds, target_names=list(label2id.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded:\n",
      "label2id: [('healthcare|FDA', 0), ('healthcare|medicaid', 1), ('healthcare|medicare', 2)]...\n",
      "id2label: [(0, 'healthcare|FDA'), (1, 'healthcare|medicaid'), (2, 'healthcare|medicare')]...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"/Users/gwin/Documents/Post Undergrad Work/Tax Search/models/model_20250411_122546\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load label mappings - FIXED VERSION\n",
    "with open(f\"{model_path}/label_mappings.json\", \"r\") as f:\n",
    "    mappings = json.load(f)  # Load ONCE\n",
    "    label2id = mappings[\"label2id\"]\n",
    "    id2label = {int(k): v for k, v in mappings[\"id2label\"].items()}  # Convert keys to int\n",
    "\n",
    "print(\"Successfully loaded:\")\n",
    "print(f\"label2id: {list(label2id.items())[:3]}...\")  # Print first 3 as sample\n",
    "print(f\"id2label: {list(id2label.items())[:3]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tokenize validation data\n",
    "val_encodings = tokenizer(\n",
    "    val_df[\"text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**val_encodings)\n",
    "    preds = np.argmax(outputs.logits.numpy(), axis=-1)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(\n",
    "    val_df[\"label\"].map(label2id).values,  # True labels (numeric)\n",
    "    preds,\n",
    "    target_names=list(label2id.keys())\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# model_path = \"/Users/gwin/Documents/Post Undergrad Work/Tax Search/models/model_20250411_122546\"  # Replace with your actual path\n",
    "# mapping_file = f\"{model_path}/label_mappings.json\"\n",
    "\n",
    "# # Check if file exists and has content\n",
    "# if not os.path.exists(mapping_file):\n",
    "#     raise FileNotFoundError(f\"Label mapping file not found at {mapping_file}\")\n",
    "\n",
    "# if os.path.getsize(mapping_file) == 0:\n",
    "#     raise ValueError(\"Label mapping file is empty\")\n",
    "\n",
    "# import json\n",
    "\n",
    "# try:\n",
    "#     with open(mapping_file, \"r\") as f:\n",
    "#         mappings = json.load(f)\n",
    "#         label2id = mappings[\"label2id\"]\n",
    "#         id2label = {int(k): v for k, v in mappings[\"id2label\"].items()}  # Ensure keys are int\n",
    "# except json.JSONDecodeError as e:\n",
    "#     print(f\"Invalid JSON in {mapping_file}: {e}\")\n",
    "#     print(\"File content:\", open(mapping_file).read())\n",
    "#     raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
