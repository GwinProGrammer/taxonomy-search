{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 272 PDFs to process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e542519a5b14cb6991c2b5308b22e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing PDFs:   0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete! Check data/cleaned/ for results.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # Progress bars\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "RAW_DATA_DIR = Path(\"data/raw\") \n",
    "CLEANED_DIR = Path(\"data/cleaned\")\n",
    "ERROR_LOG = \"cleaning_errors.csv\"\n",
    "\n",
    "# --- Cleaning Functions ---\n",
    "def clean_text(text):\n",
    "    \"\"\"Applies all cleaning rules to extracted text\"\"\"\n",
    "    # Fix hyphenated line breaks\n",
    "    text = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text, flags=re.UNICODE)\n",
    "    # Remove common PDF artifacts\n",
    "    text = re.sub(r\"Page\\s*\\d+\\s*of\\s*\\d+\", \"\", text, flags=re.IGNORECASE)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    \"\"\"Extracts and cleans text from a single PDF\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            pages = [clean_text(page.extract_text(layout=True)) \n",
    "                   for page in pdf.pages if page.extract_text()]\n",
    "        return \" \".join(pages), None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "# --- Directory Structure Setup ---\n",
    "def mirror_directory_structure(base_dir, target_dir):\n",
    "    \"\"\"Creates matching subdirectories in target location\"\"\"\n",
    "    for path in base_dir.rglob(\"*\"):\n",
    "        if path.is_dir():\n",
    "            relative = path.relative_to(base_dir)\n",
    "            (target_dir / relative).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Main Processing ---\n",
    "def process_all_pdfs():\n",
    "    errors = []\n",
    "    mirror_directory_structure(RAW_DATA_DIR, CLEANED_DIR)\n",
    "    \n",
    "    pdf_paths = list(RAW_DATA_DIR.rglob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_paths)} PDFs to process\")\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_paths, desc=\"Processing PDFs\"):\n",
    "        text, error = process_pdf(pdf_path)\n",
    "        relative_path = pdf_path.relative_to(RAW_DATA_DIR)\n",
    "        \n",
    "        if text:\n",
    "            # Save cleaned text\n",
    "            output_path = CLEANED_DIR / relative_path.with_suffix(\".txt\")\n",
    "            output_path.write_text(text, encoding=\"utf-8\")\n",
    "        elif error:\n",
    "            errors.append({\n",
    "                \"file\": str(relative_path),\n",
    "                \"error\": error\n",
    "            })\n",
    "    \n",
    "    # Save error log\n",
    "    if errors:\n",
    "        pd.DataFrame(errors).to_csv(ERROR_LOG, index=False)\n",
    "        print(f\"Encountered {len(errors)} errors - see {ERROR_LOG}\")\n",
    "\n",
    "# Execute\n",
    "process_all_pdfs()\n",
    "print(\"Cleaning complete! Check data/cleaned/ for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Medicaid Annual Report - 2024.txt ===\n",
      "Department of Health and Hu December 1, 2024 The Honorable Jim Pillen Governor of Nebraska P.O. Box 94848 Lincoln, NE 68509 Mr. Brandon Metzler Clerk of the Legislature P.O. Box 94604 Lincoln, NE 6850...\n",
      "\n",
      "\n",
      "=== R87MCM.txt ===\n",
      "CMSManual System Department of Health & Human Services (DHHS) Pub. 100-16 Medicare Managed Care Centers for Medicare & Medicaid Services (CMS) Transmittal 87 Date: JUNE 8, 2007 SUBJECT: Update of Chap...\n",
      "\n",
      "\n",
      "=== draft-exhibits-part21.txt ===\n",
      "Annual Financial Report (AFR) -2017 Other Assets Latest Fiscal Year End Previous Fiscal Year End Deposits $0.00 Other Prepaid Expenses $0.00 Goodwill $0.00 Revolving Book Account $0.00 SAMatching Fund...\n",
      "\n",
      "\n",
      "=== Budget-in-Brief 2024.txt ===\n",
      "VILLAGEOFWOODRIDGEBUDGET-IN-BRIEFFISCALYEAR 2024 MAYORALetter from the Mayor Gina Cunningham Village of Woodridge Residents and Businesses: BOARDOFTRUSTEES Jennifer Anteliz It is my pleasure to pres...\n",
      "\n",
      "\n",
      "=== p334.txt ===\n",
      "...\n",
      "\n",
      "\n",
      "=== mc86c04.txt ===\n",
      "Medicare Managed Care Manual Chapter 4 - Benefits and Beneficiary Protections Table of Contents (Rev. 121, Issued: 04-22-16) Transmittals for Chapter 4 10 – Introduction 10.1 – General Requirements 10...\n",
      "\n",
      "\n",
      "=== 20160322_R44427_ffcd0d86c54b5076bfaffc6ada8f95154ecd9a07.txt ===\n",
      "Cybersecurity: Federal Government Authoritative Reports and Resources name redac ted Senior Research Librarian March 22, 2016 Congressional Research Service 7-.... www.crs.gov R44427 Cybersecurity: Fe...\n",
      "\n",
      "\n",
      "=== FINRED-PublicServiceLoanForgiveness-FS.txt ===\n",
      "Understanding the Public Service Loan Forgiveness Program The Public Service Loan Forgiveness (PSLF) Program forgives qualifying federal direct loans of eligible borrowers who make 120 qualifying paym...\n",
      "\n",
      "\n",
      "=== R115MCM.txt ===\n",
      "CMSManual System Department of Health & Human Services (DHHS) Pub. 100-16 Medicare Managed Care Centers for Medicare & Medicaid Services (CMS) Transmittal 115 Date: August 23, 2013 SUBJECT: Chapter 4,...\n",
      "\n",
      "\n",
      "=== A-136-for-FY-2023.txt ===\n",
      "EXECUTIVEOFFICEOFTHEPRESIDENTOFFICEOFMANAGEMENTANDBUDGETWASHINGTON, D.C. 20503 THEDIRECTORMay 19, 2023 CIRCULARA-136 Revised TOTHEHEADSOFEXECUTIVEDEPARTMENTS, AGENCIES, ANDOTHERENTITIESSUBJECTTOTHECHI...\n",
      "\n",
      "\n",
      "=== IF12383.1.txt ===\n",
      "April 20, 2023 U.S. Environmental Protection Agency (EPA) Appropriations: FY2024 President’s Budget Request Since FY2006, Congress has funded the U.S. Figure 2. EPATotal Discretionary Budget Authority...\n",
      "\n",
      "\n",
      "=== 2024301.txt ===\n",
      "Common Core of Data Revenues and Expenditures for Public Elementary and Secondary Education: School Year 2021–22 (Fiscal Year 2022) 2024-301 First Look Report U.S. DEPARTMENTOFEDUCATIONAPublication of...\n",
      "\n",
      "\n",
      "=== ED660334.txt ===\n",
      "Closing the College Affordability Gap December 2023 A report for The Institute for College Access and Success (TICAS) authored by Donald E. Heller. Introduction The role that finances play as a barrie...\n",
      "\n",
      "\n",
      "=== FR-1996-11-29.txt ===\n",
      "retsiger laredef 11–29–96 Friday Vol. 61 No. 231 November 29, 1996 Pages 60509–63690...\n",
      "\n",
      "\n",
      "=== FDA-Drug-Safety-Communication--Fluoroquinolone.txt ===\n",
      "FDADrug Safety Communication FDA reinforces safety information about serious low blood sugar levels and mental health side effects with fluoroquinolone antibiotics; requires label changes Safety Annou...\n",
      "\n",
      "\n",
      "=== n-24-13.txt ===\n",
      "Part I - Administrative, Procedural, and Miscellaneous Request for Comments on Product Identification Numbers and the Energy Efficient Home Improvement Credit under Section 25CNotice 2024-13 SECTION 1...\n",
      "\n",
      "\n",
      "=== dsc_update_ocaliva_risk_of_serious_liver_injury.txt ===\n",
      "FDADrug Safety Communication This information is an update to the FDADrug Safety Communication: Due to risk of serious liver injury, FDA restricts use of Ocaliva (obeticholic acid) in primary biliary ...\n",
      "\n",
      "\n",
      "=== jer.2013.8.5.40.txt ===\n",
      "40 H. Silverman, H. Edwards, A. Shamoo, A. Matar Enhancing Research Ethics Capacity in the Middle East: Experience and Challenges of a Fogarty-Sponsored Training Program Henry Silverman, Hillary Edwar...\n",
      "\n",
      "\n",
      "=== OMB-Bulletin-No.-19-03-Audit-Requirements-for-Federal-Financial-Statements.txt ===\n",
      "EXECUTIVEOFFICEOFTHEPRESIDENTOFFICEOFMANAGEMENTANDBUDGETWASHINGTON, D.C. 20503 August 27, 2019 0MBBULLETINNO. 19-03 T0 THEHEADS, INSPECTORSGENERAL, ANDCHIEFFINANCIALOFFICERSOFEXECUTIVEDEPARTMENTSANDES...\n",
      "\n",
      "\n",
      "=== December-8-2023-Approval-Letter-LYFGENIA.txt ===\n",
      "Our STN: BL 125788/0 BLAAPPROVALDecember 08, 2023 bluebird bio, Inc. Attention: Megan Parsi, MBS 455 Grand Union Boulevard Somerville, MA 02145 Dear Ms. Parsi: Please refer to your Biologics License A...\n",
      "\n",
      "\n",
      "=== ccr_2620-5_-_isr.txt ===\n",
      "CALIFORNIAARCHITECTSBOARDLANDSCAPEARCHITECTSTECHNICALCOMMITTEEINITIALSTATEMENTOFREASONSHearing Date: August 6, 2012 Subject Matter of Proposed Regulation: Requirements for an Approved Extension Certif...\n",
      "\n",
      "\n",
      "=== 09-085.txt ===\n",
      "D2009-085 June 8, 2009 Contracting for Nontactical Vehicles in Support of Operation Enduring Freedom Additional Information and Copies To obtain...\n",
      "\n",
      "\n",
      "=== ess-framework-implementation-guide-2015-508.txt ===\n",
      "Foreword The National Institute of Standards and Technology (NIST) released the 2014 Framework for Improving Critical Infrastructure Cybersecurity (Framework) as a voluntary, risk-based set of standar...\n",
      "\n",
      "\n",
      "=== FY-2018-Annual-Report-Final-508-V2.txt ===\n",
      "Fiscal Year 2018 Annual Report United States Department of Education Betsy DeVos Secretary Federal Student Aid James F. Manning Acting Chief Operating Officer Finance Office Alison L. Doone Chief Fina...\n",
      "\n",
      "\n",
      "=== HPMS_D_SNP_and_PACE_Medicaid_Unwinding_G.txt ===\n",
      "DEPARTMENTOFHEALTH & HUMANSERVICESCenters for Medicare & Medicaid Services 7500 Security Boulevard Baltimore, Maryland 21244-1850 DATE: June 7, 2023 TO: Medicare Advantage Dual Eligible Special Needs ...\n",
      "\n",
      "\n",
      "Total cleaned files: 269\n",
      "Avg. chars per file: 39726\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Get 3 random cleaned files\n",
    "cleaned_files = list(Path(\"data/cleaned\").rglob(\"*.txt\"))\n",
    "samples = random.sample(cleaned_files, 25)\n",
    "\n",
    "for file in samples:\n",
    "    print(f\"=== {file.name} ===\")\n",
    "    print(file.read_text()[:200] + \"...\")  # First 200 chars\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(f\"Total cleaned files: {len(cleaned_files)}\")\n",
    "avg_length = sum(len(f.read_text()) for f in cleaned_files)/len(cleaned_files)\n",
    "print(f\"Avg. chars per file: {avg_length:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cleaned 269 files\n",
      "Structure preserved in:\n",
      "  data/cleaned/[category]/[subcategory]/*.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Enhanced cleaning with document structure awareness\"\"\"\n",
    "    # Phase 1: Remove document metadata patterns\n",
    "    patterns_to_remove = [\n",
    "        # Header/footer patterns\n",
    "        r'^=== .*? ===\\s*',  \n",
    "        r'^OFFICE OF .*?$\\n',\n",
    "        r'^Department of .*?$\\n',\n",
    "        \n",
    "        # Author patterns (expanded list)\n",
    "        r'^(?:Authored?|Written|Prepared|Reported|Compiled) by .*?$\\n',\n",
    "        r'^Contributors?:.*?$\\n',\n",
    "        r'^(?:The )?Authors?:?.*?$\\n',\n",
    "        r'^By [A-Z][a-z]+ [A-Z][a-z]+(?:,? (?:and|&) [A-Z][a-z]+ [A-Z][a-z]+)*$\\n',\n",
    "        \n",
    "        # Boilerplate text\n",
    "        r'Additional Copies.*',\n",
    "        r'For more information.*',\n",
    "        r'Distribution Statement.*'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns_to_remove:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE|re.MULTILINE)\n",
    "    \n",
    "    # Phase 2: Clean residual formatting\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Reduce excessive newlines\n",
    "    text = re.sub(r'[^\\S\\n]{2,}', ' ', text)  # Fix multiple spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_category_directory(base_dir):\n",
    "    \"\"\"Process all files in data/cleaned/[category]/[subcategory] structure\"\"\"\n",
    "    base_path = Path(base_dir)\n",
    "    total_files = 0\n",
    "    \n",
    "    for category in base_path.iterdir():\n",
    "        if not category.is_dir():\n",
    "            continue\n",
    "            \n",
    "        for subcategory in category.iterdir():\n",
    "            if not subcategory.is_dir():\n",
    "                continue\n",
    "                \n",
    "            for file in subcategory.glob('*.txt'):\n",
    "                total_files += 1\n",
    "                with open(file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    original = f.read()\n",
    "                \n",
    "                cleaned = clean_text(original)\n",
    "                \n",
    "                # Overwrite with cleaned version (or use output_dir to preserve originals)\n",
    "                with open(file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(cleaned)\n",
    "    \n",
    "    return total_files\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_base = 'data/cleaned'  # Your exact structure\n",
    "    processed_count = process_category_directory(input_base)\n",
    "    \n",
    "    print(f\"Successfully cleaned {processed_count} files\")\n",
    "    print(\"Structure preserved in:\")\n",
    "    print(f\"  {input_base}/[category]/[subcategory]/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ✅ FINAL CLEANING COMPLETE\n",
      "    =========================\n",
      "    Files processed: 269\n",
      "    Government documents: 268\n",
      "    Files with tax metadata fixed: 9\n",
      "    \n",
      "    Full report saved to: cleaning_report.csv\n",
      "    \n",
      "    NEXT STEPS:\n",
      "    1. Review 'is_government=False' files in the report\n",
      "    2. Delete any remaining non-gov files manually\n",
      "    3. Proceed to model training\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration - Only edit these patterns if absolutely needed\n",
    "PATTERNS = {\n",
    "    'tax_forms': r'Userid: .*?cycle\\d+/source.*?(\\n|$)',  # IRS/CMS system metadata\n",
    "    'corrupted_caps': r'([A-Z])\\1{2,}',  # Fix \"EEXXEECCUUTTIIVVEE\" → \"EXECUTIVE\"\n",
    "    'non_gov': [  # Phrases that indicate non-government content\n",
    "        r'opinion piece',\n",
    "        r'charitable giving',\n",
    "        r'by [A-Z][a-z]+ [A-Z][a-z]+, [A-Za-z ]+ Foundation'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def final_clean(text, filepath):\n",
    "    \"\"\"One-time aggressive cleaning with validation\"\"\"\n",
    "    # Step 1: Remove known bad patterns\n",
    "    text = re.sub(PATTERNS['tax_forms'], '', text, flags=re.DOTALL|re.IGNORECASE)\n",
    "    text = re.sub(PATTERNS['corrupted_caps'], r'\\1', text)\n",
    "    \n",
    "    # Step 2: Flag (not delete) non-government content\n",
    "    is_gov = not any(re.search(p, text, re.I) for p in PATTERNS['non_gov'])\n",
    "    \n",
    "    # Step 3: Final normalization\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text).strip()\n",
    "    return text, is_gov\n",
    "\n",
    "def main(input_dir=\"data/cleaned\"):\n",
    "    \"\"\"Run final clean with validation report\"\"\"\n",
    "    stats = []\n",
    "    for filepath in Path(input_dir).rglob(\"*.txt\"):\n",
    "        original = filepath.read_text(encoding='utf-8', errors='replace')\n",
    "        cleaned, is_gov = final_clean(original, filepath)\n",
    "        \n",
    "        stats.append({\n",
    "            'file': str(filepath),\n",
    "            'original_lines': len(original.splitlines()),\n",
    "            'cleaned_lines': len(cleaned.splitlines()),\n",
    "            'is_government': is_gov,\n",
    "            'issues_fixed': \"tax_forms\" if \"Userid: CPMSchema\" in original else None\n",
    "        })\n",
    "        \n",
    "        filepath.write_text(cleaned)\n",
    "    \n",
    "    # Generate validation report\n",
    "    df = pd.DataFrame(stats)\n",
    "    report_path = Path(\"cleaning_report.csv\")\n",
    "    df.to_csv(report_path, index=False)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    ✅ FINAL CLEANING COMPLETE\n",
    "    =========================\n",
    "    Files processed: {len(df)}\n",
    "    Government documents: {df['is_government'].sum()}\n",
    "    Files with tax metadata fixed: {sum(df['issues_fixed'] == \"tax_forms\")}\n",
    "    \n",
    "    Full report saved to: {report_path}\n",
    "    \n",
    "    NEXT STEPS:\n",
    "    1. Review 'is_government=False' files in the report\n",
    "    2. Delete any remaining non-gov files manually\n",
    "    3. Proceed to model training\n",
    "    \"\"\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV created with 269 documents\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data = []\n",
    "for file in Path(\"data/cleaned\").rglob(\"*.txt\"):\n",
    "    category = file.parent.parent.name  # healthcare\n",
    "    subcategory = file.parent.name      # medicaid\n",
    "    text = file.read_text(encoding='utf-8').strip()\n",
    "    \n",
    "    data.append({\n",
    "        \"text\": text,\n",
    "        \"category\": category,\n",
    "        \"subcategory\": subcategory,\n",
    "        \"source_file\": file.name\n",
    "    })\n",
    "\n",
    "pd.DataFrame(data).to_csv(\"gov_docs.csv\", index=False)\n",
    "print(\"✅ CSV created with\", len(data), \"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text   category  \\\n",
      "224  FCritical Manufacturing Cybersecurity Framewor...    defense   \n",
      "133  Revenues and Expenditures for Public FINANCETA...  education   \n",
      "106  Glossary CFRDCLCost of Attendance 2 CHAPTER (B...  education   \n",
      "\n",
      "               subcategory                                        source_file  \n",
      "224          cybersecurity  critical-manufacturing-framework-implementatio...  \n",
      "133             k12funding                                        2022301.txt  \n",
      "106  highereducationpolicy  2019-2020 Chapter 2 - Cost of Attendance (Budg...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"gov_docs.csv\")\n",
    "print(df.sample(3))  # Spot-check entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myour_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m hierarchy_issues \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(hierarchy_issues[hierarchy_issues]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/taxo/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/taxo/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/taxo/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/taxo/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/taxo/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"gov_docs.csv\")\n",
    "hierarchy_issues = df.groupby(\"subcategory\")[\"category\"].nunique() > 1\n",
    "print(hierarchy_issues[hierarchy_issues].index.tolist())  # Bad subcategories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
