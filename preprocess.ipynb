{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 272 PDFs to process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e542519a5b14cb6991c2b5308b22e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing PDFs:   0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete! Check data/cleaned/ for results.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # Progress bars\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "RAW_DATA_DIR = Path(\"data/raw\") \n",
    "CLEANED_DIR = Path(\"data/cleaned\")\n",
    "ERROR_LOG = \"cleaning_errors.csv\"\n",
    "\n",
    "# --- Cleaning Functions ---\n",
    "def clean_text(text):\n",
    "    \"\"\"Applies all cleaning rules to extracted text\"\"\"\n",
    "    # Fix hyphenated line breaks\n",
    "    text = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text, flags=re.UNICODE)\n",
    "    # Remove common PDF artifacts\n",
    "    text = re.sub(r\"Page\\s*\\d+\\s*of\\s*\\d+\", \"\", text, flags=re.IGNORECASE)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    \"\"\"Extracts and cleans text from a single PDF\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            pages = [clean_text(page.extract_text(layout=True)) \n",
    "                   for page in pdf.pages if page.extract_text()]\n",
    "        return \" \".join(pages), None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "# --- Directory Structure Setup ---\n",
    "def mirror_directory_structure(base_dir, target_dir):\n",
    "    \"\"\"Creates matching subdirectories in target location\"\"\"\n",
    "    for path in base_dir.rglob(\"*\"):\n",
    "        if path.is_dir():\n",
    "            relative = path.relative_to(base_dir)\n",
    "            (target_dir / relative).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Main Processing ---\n",
    "def process_all_pdfs():\n",
    "    errors = []\n",
    "    mirror_directory_structure(RAW_DATA_DIR, CLEANED_DIR)\n",
    "    \n",
    "    pdf_paths = list(RAW_DATA_DIR.rglob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_paths)} PDFs to process\")\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_paths, desc=\"Processing PDFs\"):\n",
    "        text, error = process_pdf(pdf_path)\n",
    "        relative_path = pdf_path.relative_to(RAW_DATA_DIR)\n",
    "        \n",
    "        if text:\n",
    "            # Save cleaned text\n",
    "            output_path = CLEANED_DIR / relative_path.with_suffix(\".txt\")\n",
    "            output_path.write_text(text, encoding=\"utf-8\")\n",
    "        elif error:\n",
    "            errors.append({\n",
    "                \"file\": str(relative_path),\n",
    "                \"error\": error\n",
    "            })\n",
    "    \n",
    "    # Save error log\n",
    "    if errors:\n",
    "        pd.DataFrame(errors).to_csv(ERROR_LOG, index=False)\n",
    "        print(f\"Encountered {len(errors)} errors - see {ERROR_LOG}\")\n",
    "\n",
    "# Execute\n",
    "process_all_pdfs()\n",
    "print(\"Cleaning complete! Check data/cleaned/ for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== medicaid-chip-overview-webinar-sept-2024-final-508.txt ===\n",
      "Medicaid and CHIPOverview September 2024 Thisinformationisintendedonlyfortheuseofentitiesandindividualscertified to serve as Navigators, certified application counselors, or non-Navigator personnel in...\n",
      "\n",
      "\n",
      "=== 09-085.txt ===\n",
      "D2009-085 June 8, 2009 Contracting for Nontactical Vehicles in Support of Operation Enduring Freedom Additional Information and Copies To obtain...\n",
      "\n",
      "\n",
      "=== Medicare-Fee-For-Service-Getting-Payments-Right-Scorecard-FY-2019-Q2.txt ===\n",
      "Goal: Getting Payments Right Change from Previous FY ($M) Cash Loss by FY ($M) $15,000M $10,000M ## KKeeyy MMiilleessttoonneess MilesSttoantuess S.. ECD 11 FFiinnaalliizzee e esstitmimaatetde dc acsah...\n",
      "\n",
      "\n",
      "=== Course_7-FAQ_Medicare_Advantage_3-16-2022.txt ===\n",
      "Centers for Medicare & Medicaid Services National Training Program Frequently Asked Questions (FAQ): Medicare Advantage 1. Original Medicare & Medicare Advantage What are the differences between Origi...\n",
      "\n",
      "\n",
      "=== December 8, 2023 Approval Letter - CASGEVY.txt ===\n",
      "(~ Ii U.S. FOOD & DRUG °'¾--::i~ ADMINISTRATIONOur STN: BL 125787/0 BLAAPPROVALDecember 08, 2023 Vertex Pharmaceuticals Inc Attention: Brett Richardson 50 Northern Avenue Boston, MA 02210 Dear Mr. Ric...\n",
      "\n",
      "\n",
      "=== Updated GAO Testimony.txt ===\n",
      "United States Government Accountability Office Testimony Before the Subcommittee on Readiness a nd Management Support, Committee on Armed Services, U.S. Senate For Release on Delivery DODACQUISITIONEx...\n",
      "\n",
      "\n",
      "=== jun19_ch11_medpac_reporttocongress_sec.txt ===\n",
      "11 CHAPTEROptions for slowing the growth of Medicare fee-for-service spending for emergency department services RECOMMENDATION 11 The Secretary should develop and implement a set of national guideline...\n",
      "\n",
      "\n",
      "=== 2022 Medicare Fee-for-Service Supplemental Improper Payment Data.txt ===\n",
      "U.S. DEPARTMENTOFHEALTH & HUMANSERVICES 2022 Medicare Fee-for- Service Supplemental Improper Payment Data TABLEOFCONTENTSSummary of High Level Findings ...................................................\n",
      "\n",
      "\n",
      "=== MedicareMedicaidSummaries2009.txt ===\n",
      "BRIEFSUMMARIES of MEDICARE & MEDICAIDTitle XVI and Title XIX of The Social Security Act as of November 1, 2009...\n",
      "\n",
      "\n",
      "=== macpro-ig-submission-medicaid-state-plan.txt ===\n",
      "Implementation Guide: Submission – Medicaid State Plan Contents BACKGROUND ............................................................................................................ 2 INSTRUCTIONS ....\n",
      "\n",
      "\n",
      "=== Intercept Ocaliva DSC Company Statement_March 28 2025.txt ===\n",
      "Acknowledgement of FDADrug Safety Communication March 28, 2025 – ADrug Safety Communication (DSC) issued by the USFood and Drug Administration (FDA) in December 2024 advises healthcare professionals t...\n",
      "\n",
      "\n",
      "=== 06-123.txt ===\n",
      "September 29, 2006 Acquisition Program Management of the Objective Individual Combat Weapon Increment I (D-2006-123) This special version of the report has been revised to omit attorney client privile...\n",
      "\n",
      "\n",
      "=== 07-130.txt ===\n",
      "Report No. D2007-130 September 28, 2007 Contracting Practices at Air Force Laboratory Facilities...\n",
      "\n",
      "\n",
      "=== HPMS_D_SNP_and_PACE_Medicaid_Unwinding_G.txt ===\n",
      "DEPARTMENTOFHEALTH & HUMANSERVICESCenters for Medicare & Medicaid Services 7500 Security Boulevard Baltimore, Maryland 21244-1850 DATE: June 7, 2023 TO: Medicare Advantage Dual Eligible Special Needs ...\n",
      "\n",
      "\n",
      "=== p583.txt ===\n",
      "...\n",
      "\n",
      "\n",
      "=== FDA-Drug-Safety-Communication--Fluoroquinolone.txt ===\n",
      "FDADrug Safety Communication FDA reinforces safety information about serious low blood sugar levels and mental health side effects with fluoroquinolone antibiotics; requires label changes Safety Annou...\n",
      "\n",
      "\n",
      "=== FY2024-BIB-Introduction.txt ===\n",
      "The Department of Commerce Budget in Brief Fiscal Year 2024 Gina M. Raimondo, Sec retary Contents Contents OVERVIEW 1 BUREAUDESCRIPTIONSDepartmental Management 6 Office of the Inspector General 24 Eco...\n",
      "\n",
      "\n",
      "=== n-24-13.txt ===\n",
      "Part I - Administrative, Procedural, and Miscellaneous Request for Comments on Product Identification Numbers and the Energy Efficient Home Improvement Credit under Section 25CNotice 2024-13 SECTION 1...\n",
      "\n",
      "\n",
      "=== IF12383.1.txt ===\n",
      "April 20, 2023 U.S. Environmental Protection Agency (EPA) Appropriations: FY2024 President’s Budget Request Since FY2006, Congress has funded the U.S. Figure 2. EPATotal Discretionary Budget Authority...\n",
      "\n",
      "\n",
      "=== FY-2018-Annual-Report-Final-508-V2.txt ===\n",
      "Fiscal Year 2018 Annual Report United States Department of Education Betsy DeVos Secretary Federal Student Aid James F. Manning Acting Chief Operating Officer Finance Office Alison L. Doone Chief Fina...\n",
      "\n",
      "\n",
      "=== pslf-infographic.txt ===\n",
      "HOWTOGETYOURSTUDENTLOANSFORGIVEN ( NO, REALLY ) Public Service Loan Forgiveness (PSLF) is a program that could eliminate some of your student loan debt—as long as you meet all the requirements, that i...\n",
      "\n",
      "\n",
      "=== ess-framework-implementation-guide-2015-508.txt ===\n",
      "Foreword The National Institute of Standards and Technology (NIST) released the 2014 Framework for Improving Critical Infrastructure Cybersecurity (Framework) as a voluntary, risk-based set of standar...\n",
      "\n",
      "\n",
      "=== CRS_DoDIGCA.txt ===\n",
      "Inherently Governmental Functions and Department of Defense Operations: Background, Issues, and Options for Congress John R. Luckey Legislative Attorney Valerie Bailey Grasso Specialist in Defense Acq...\n",
      "\n",
      "\n",
      "=== atp6-02-71.txt ===\n",
      "ATP 6-02.71 TECHNIQUESFORDEPARTMENTOFDEFENSEINFORMATIONNETWORKOPERATIONSAPRIL 2019 DISTRIBUTIONRESTRICTION: Approved for public release; distribution is unlimited. *This publication supersedes FM 6-02...\n",
      "\n",
      "\n",
      "=== smd23001.txt ===\n",
      "DEPARTMENTOFHEALTH & HUMANSERVICESCenters for Medicare & Medicaid Services 7500 Security Boulevard, Mail Stop S2-26-12 Baltimore, Maryland 21244-1850 SMD #: 23-001 RE: Additional Guidance on Use of In...\n",
      "\n",
      "\n",
      "Total cleaned files: 269\n",
      "Avg. chars per file: 39726\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Get 3 random cleaned files\n",
    "cleaned_files = list(Path(\"data/cleaned\").rglob(\"*.txt\"))\n",
    "samples = random.sample(cleaned_files, 25)\n",
    "\n",
    "for file in samples:\n",
    "    print(f\"=== {file.name} ===\")\n",
    "    print(file.read_text()[:200] + \"...\")  # First 200 chars\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(f\"Total cleaned files: {len(cleaned_files)}\")\n",
    "avg_length = sum(len(f.read_text()) for f in cleaned_files)/len(cleaned_files)\n",
    "print(f\"Avg. chars per file: {avg_length:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cleaned 269 files\n",
      "Structure preserved in:\n",
      "  data/cleaned/[category]/[subcategory]/*.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Enhanced cleaning with document structure awareness\"\"\"\n",
    "    # Phase 1: Remove document metadata patterns\n",
    "    patterns_to_remove = [\n",
    "        # Header/footer patterns\n",
    "        r'^=== .*? ===\\s*',  \n",
    "        r'^OFFICE OF .*?$\\n',\n",
    "        r'^Department of .*?$\\n',\n",
    "        \n",
    "        # Author patterns (expanded list)\n",
    "        r'^(?:Authored?|Written|Prepared|Reported|Compiled) by .*?$\\n',\n",
    "        r'^Contributors?:.*?$\\n',\n",
    "        r'^(?:The )?Authors?:?.*?$\\n',\n",
    "        r'^By [A-Z][a-z]+ [A-Z][a-z]+(?:,? (?:and|&) [A-Z][a-z]+ [A-Z][a-z]+)*$\\n',\n",
    "        \n",
    "        # Boilerplate text\n",
    "        r'Additional Copies.*',\n",
    "        r'For more information.*',\n",
    "        r'Distribution Statement.*'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns_to_remove:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE|re.MULTILINE)\n",
    "    \n",
    "    # Phase 2: Clean residual formatting\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Reduce excessive newlines\n",
    "    text = re.sub(r'[^\\S\\n]{2,}', ' ', text)  # Fix multiple spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def process_category_directory(base_dir):\n",
    "    \"\"\"Process all files in data/cleaned/[category]/[subcategory] structure\"\"\"\n",
    "    base_path = Path(base_dir)\n",
    "    total_files = 0\n",
    "    \n",
    "    for category in base_path.iterdir():\n",
    "        if not category.is_dir():\n",
    "            continue\n",
    "            \n",
    "        for subcategory in category.iterdir():\n",
    "            if not subcategory.is_dir():\n",
    "                continue\n",
    "                \n",
    "            for file in subcategory.glob('*.txt'):\n",
    "                total_files += 1\n",
    "                with open(file, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    original = f.read()\n",
    "                \n",
    "                cleaned = clean_text(original)\n",
    "                \n",
    "                # Overwrite with cleaned version (or use output_dir to preserve originals)\n",
    "                with open(file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(cleaned)\n",
    "    \n",
    "    return total_files\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_base = 'data/cleaned'  # Your exact structure\n",
    "    processed_count = process_category_directory(input_base)\n",
    "    \n",
    "    print(f\"Successfully cleaned {processed_count} files\")\n",
    "    print(\"Structure preserved in:\")\n",
    "    print(f\"  {input_base}/[category]/[subcategory]/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ✅ FINAL CLEANING COMPLETE\n",
      "    =========================\n",
      "    Files processed: 269\n",
      "    Government documents: 268\n",
      "    Files with tax metadata fixed: 9\n",
      "    \n",
      "    Full report saved to: cleaning_report.csv\n",
      "    \n",
      "    NEXT STEPS:\n",
      "    1. Review 'is_government=False' files in the report\n",
      "    2. Delete any remaining non-gov files manually\n",
      "    3. Proceed to model training\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration - Only edit these patterns if absolutely needed\n",
    "PATTERNS = {\n",
    "    'tax_forms': r'Userid: .*?cycle\\d+/source.*?(\\n|$)',  # IRS/CMS system metadata\n",
    "    'corrupted_caps': r'([A-Z])\\1{2,}',  # Fix \"EEXXEECCUUTTIIVVEE\" → \"EXECUTIVE\"\n",
    "    'non_gov': [  # Phrases that indicate non-government content\n",
    "        r'opinion piece',\n",
    "        r'charitable giving',\n",
    "        r'by [A-Z][a-z]+ [A-Z][a-z]+, [A-Za-z ]+ Foundation'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def final_clean(text, filepath):\n",
    "    \"\"\"One-time aggressive cleaning with validation\"\"\"\n",
    "    # Step 1: Remove known bad patterns\n",
    "    text = re.sub(PATTERNS['tax_forms'], '', text, flags=re.DOTALL|re.IGNORECASE)\n",
    "    text = re.sub(PATTERNS['corrupted_caps'], r'\\1', text)\n",
    "    \n",
    "    # Step 2: Flag (not delete) non-government content\n",
    "    is_gov = not any(re.search(p, text, re.I) for p in PATTERNS['non_gov'])\n",
    "    \n",
    "    # Step 3: Final normalization\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text).strip()\n",
    "    return text, is_gov\n",
    "\n",
    "def main(input_dir=\"data/cleaned\"):\n",
    "    \"\"\"Run final clean with validation report\"\"\"\n",
    "    stats = []\n",
    "    for filepath in Path(input_dir).rglob(\"*.txt\"):\n",
    "        original = filepath.read_text(encoding='utf-8', errors='replace')\n",
    "        cleaned, is_gov = final_clean(original, filepath)\n",
    "        \n",
    "        stats.append({\n",
    "            'file': str(filepath),\n",
    "            'original_lines': len(original.splitlines()),\n",
    "            'cleaned_lines': len(cleaned.splitlines()),\n",
    "            'is_government': is_gov,\n",
    "            'issues_fixed': \"tax_forms\" if \"Userid: CPMSchema\" in original else None\n",
    "        })\n",
    "        \n",
    "        filepath.write_text(cleaned)\n",
    "    \n",
    "    # Generate validation report\n",
    "    df = pd.DataFrame(stats)\n",
    "    report_path = Path(\"cleaning_report.csv\")\n",
    "    df.to_csv(report_path, index=False)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    ✅ FINAL CLEANING COMPLETE\n",
    "    =========================\n",
    "    Files processed: {len(df)}\n",
    "    Government documents: {df['is_government'].sum()}\n",
    "    Files with tax metadata fixed: {sum(df['issues_fixed'] == \"tax_forms\")}\n",
    "    \n",
    "    Full report saved to: {report_path}\n",
    "    \n",
    "    NEXT STEPS:\n",
    "    1. Review 'is_government=False' files in the report\n",
    "    2. Delete any remaining non-gov files manually\n",
    "    3. Proceed to model training\n",
    "    \"\"\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
