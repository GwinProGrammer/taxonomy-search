{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 272 PDFs to process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e542519a5b14cb6991c2b5308b22e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing PDFs:   0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete! Check data/cleaned/ for results.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm  # Progress bars\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "RAW_DATA_DIR = Path(\"data/raw\") \n",
    "CLEANED_DIR = Path(\"data/cleaned\")\n",
    "ERROR_LOG = \"cleaning_errors.csv\"\n",
    "\n",
    "# --- Cleaning Functions ---\n",
    "def clean_text(text):\n",
    "    \"\"\"Applies all cleaning rules to extracted text\"\"\"\n",
    "    # Fix hyphenated line breaks\n",
    "    text = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text, flags=re.UNICODE)\n",
    "    # Remove common PDF artifacts\n",
    "    text = re.sub(r\"Page\\s*\\d+\\s*of\\s*\\d+\", \"\", text, flags=re.IGNORECASE)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    \"\"\"Extracts and cleans text from a single PDF\"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            pages = [clean_text(page.extract_text(layout=True)) \n",
    "                   for page in pdf.pages if page.extract_text()]\n",
    "        return \" \".join(pages), None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "# --- Directory Structure Setup ---\n",
    "def mirror_directory_structure(base_dir, target_dir):\n",
    "    \"\"\"Creates matching subdirectories in target location\"\"\"\n",
    "    for path in base_dir.rglob(\"*\"):\n",
    "        if path.is_dir():\n",
    "            relative = path.relative_to(base_dir)\n",
    "            (target_dir / relative).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Main Processing ---\n",
    "def process_all_pdfs():\n",
    "    errors = []\n",
    "    mirror_directory_structure(RAW_DATA_DIR, CLEANED_DIR)\n",
    "    \n",
    "    pdf_paths = list(RAW_DATA_DIR.rglob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_paths)} PDFs to process\")\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_paths, desc=\"Processing PDFs\"):\n",
    "        text, error = process_pdf(pdf_path)\n",
    "        relative_path = pdf_path.relative_to(RAW_DATA_DIR)\n",
    "        \n",
    "        if text:\n",
    "            # Save cleaned text\n",
    "            output_path = CLEANED_DIR / relative_path.with_suffix(\".txt\")\n",
    "            output_path.write_text(text, encoding=\"utf-8\")\n",
    "        elif error:\n",
    "            errors.append({\n",
    "                \"file\": str(relative_path),\n",
    "                \"error\": error\n",
    "            })\n",
    "    \n",
    "    # Save error log\n",
    "    if errors:\n",
    "        pd.DataFrame(errors).to_csv(ERROR_LOG, index=False)\n",
    "        print(f\"Encountered {len(errors)} errors - see {ERROR_LOG}\")\n",
    "\n",
    "# Execute\n",
    "process_all_pdfs()\n",
    "print(\"Cleaning complete! Check data/cleaned/ for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 2023-073-R-AssistantDDofFBC&P.txt ===\n",
      "POSTING #: ISSUE DATE: CLOSING DATE: 2023 - 073 - R July 24, 2023 September 25, 2023 TITLE: OPEN TO: FUNCTIONAL TITLE: Assistant Division Director RANGE: General Public WORKWEEK: Assistant Director of...\n",
      "\n",
      "\n",
      "=== R107MCM.txt ===\n",
      "CMS Manual System Department of Health & Human Services (DHHS) Pub. 100-16 Medicare Managed Care Centers for Medicare & Medicaid Services (CMS) Transmittal 107 Date: June 22, 2012 SUBJECT: Chapter 4, ...\n",
      "\n",
      "\n",
      "=== ED603154.txt ===\n",
      "H.R. 4674, the College Affordability Act: Proposed Reauthorization of the Higher Education Act, Summary of Major Provisions January 7, 2020 Congressional Research Service https://crsreports.congress.g...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Get 3 random cleaned files\n",
    "cleaned_files = list(Path(\"data/cleaned\").rglob(\"*.txt\"))\n",
    "samples = random.sample(cleaned_files, 3)\n",
    "\n",
    "for file in samples:\n",
    "    print(f\"=== {file.name} ===\")\n",
    "    print(file.read_text()[:200] + \"...\")  # First 200 chars\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(f\"Total cleaned files: {len(cleaned_files)}\")\n",
    "avg_length = sum(len(f.read_text()) for f in cleaned_files)/len(cleaned_files)\n",
    "print(f\"Avg. chars per file: {avg_length:.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
