{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7521aa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1: education|k12funding (P=0.12)\n",
      "L2: education|k12funding (P=0.12)\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "\n",
    "# class TaxonomyPredictor:\n",
    "#     def __init__(self, model_path=\"models/model_20250411_122546\"):\n",
    "#         \"\"\"Initialize model, tokenizer, and label mappings.\"\"\"\n",
    "#         self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "#         # Load label mappings\n",
    "#         with open(f\"{model_path}/label_mappings.json\", \"r\") as f:\n",
    "#             mappings = json.load(f)\n",
    "#             self.id2label = {int(k): v for k, v in mappings[\"id2label\"].items()}\n",
    "#             self.label_hierarchy = mappings.get(\"hierarchy\", {})  # Optional: If you have parent-child relationships\n",
    "\n",
    "#     def predict_taxonomy(self, text, top_k=3):\n",
    "#         \"\"\"Predict L1/L2 categories with probabilities.\n",
    "#         Returns:\n",
    "#             dict: {\n",
    "#                 'l1_category': str,\n",
    "#                 'l2_category': str,\n",
    "#                 'l1_prob': float,\n",
    "#                 'l2_prob': float,\n",
    "#                 'full_distribution': list  # All predictions (sorted)\n",
    "#             }\n",
    "#         \"\"\"\n",
    "#         inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             outputs = self.model(**inputs)\n",
    "#             probs = torch.softmax(outputs.logits, dim=1).numpy()[0]\n",
    "        \n",
    "#         # Get all predictions\n",
    "#         predictions = [\n",
    "#             {\"label\": self.id2label[label_id], \"score\": float(prob)}\n",
    "#             for label_id, prob in enumerate(probs)\n",
    "#         ]\n",
    "#         predictions.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        \n",
    "#         # Extract L1/L2 (assuming labels follow \"L1 > L2\" format)\n",
    "#         top_pred = predictions[0]['label']\n",
    "#         if ' > ' in top_pred:\n",
    "#             l1, l2 = top_pred.split(' > ', 1)\n",
    "#         else:\n",
    "#             l1, l2 = top_pred, top_pred  # Fallback if no hierarchy\n",
    "        \n",
    "#         return {\n",
    "#             'l1_category': l1,\n",
    "#             'l2_category': l2,\n",
    "#             'l1_prob': predictions[0]['score'],\n",
    "#             'l2_prob': predictions[0]['score'],  # Or customize logic for L2\n",
    "#             'full_distribution': predictions[:top_k]\n",
    "#         }\n",
    "\n",
    "# # --- Usage Example ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize predictor\n",
    "#     predictor = TaxonomyPredictor()\n",
    "    \n",
    "#     # Test prediction\n",
    "#     text = \"someone needs to look after my grandma\"\n",
    "#     result = predictor.predict_taxonomy(text)\n",
    "#     print(f\"L1: {result['l1_category']} (P={result['l1_prob']:.2f})\")\n",
    "#     print(f\"L2: {result['l2_category']} (P={result['l2_prob']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a27e350f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: 'someone needs to look after my grandma'\n",
      "→ Combined: education|k12funding|education|k12funding\n",
      "→ L1: education|k12funding (P=0.12)\n",
      "→ L2: education|k12funding (P=0.12)\n",
      "\n",
      "Text: 'medicare coverage for seniors'\n",
      "→ Combined: healthcare|medicare|healthcare|medicare\n",
      "→ L1: healthcare|medicare (P=0.28)\n",
      "→ L2: healthcare|medicare (P=0.28)\n",
      "\n",
      "Text: 'FDA drug approval process'\n",
      "→ Combined: healthcare|fda|healthcare|fda\n",
      "→ L1: healthcare|fda (P=0.35)\n",
      "→ L2: healthcare|fda (P=0.35)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class TaxonomyPredictor:\n",
    "    def __init__(self, model_path=\"models/model_20250411_122546\"):\n",
    "        \"\"\"Initialize model, tokenizer, and label mappings.\"\"\"\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Load label mappings\n",
    "        with open(f\"{model_path}/label_mappings.json\", \"r\") as f:\n",
    "            mappings = json.load(f)\n",
    "            self.id2label = {int(k): v for k, v in mappings[\"id2label\"].items()}\n",
    "            self.label_hierarchy = mappings.get(\"hierarchy\", {})  # Optional: If you have parent-child relationships\n",
    "\n",
    "    def predict_taxonomy(self, text, top_k=3):\n",
    "        \"\"\"Predict L1/L2 categories with probabilities.\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'l1_category': str,       # Main category (e.g., \"healthcare\")\n",
    "                'l2_category': str,       # Subcategory (e.g., \"medicare\")\n",
    "                'combined_category': str, # Combined format \"healthcare|medicare\"\n",
    "                'l1_prob': float,\n",
    "                'l2_prob': float,\n",
    "                'full_distribution': list  # All predictions (sorted)\n",
    "            }\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=1).numpy()[0]\n",
    "        \n",
    "        # Get all predictions\n",
    "        predictions = [\n",
    "            {\"label\": self.id2label[label_id], \"score\": float(prob)}\n",
    "            for label_id, prob in enumerate(probs)\n",
    "        ]\n",
    "        predictions.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        \n",
    "        # Extract L1/L2 from the predicted label\n",
    "        top_pred = predictions[0]['label']\n",
    "        if ' > ' in top_pred:\n",
    "            l1, l2 = top_pred.split(' > ', 1)\n",
    "            combined = f\"{l1.lower()}|{l2.lower()}\"  # Format as \"healthcare|medicare\"\n",
    "        else:\n",
    "            l1 = l2 = top_pred\n",
    "            combined = f\"{l1.lower()}|{l2.lower()}\"  # Fallback format\n",
    "        \n",
    "        return {\n",
    "            'l1_category': l1.lower(),\n",
    "            'l2_category': l2.lower(),\n",
    "            'combined_category': combined,  # New field with pipeline format\n",
    "            'l1_prob': predictions[0]['score'],\n",
    "            'l2_prob': predictions[0]['score'],\n",
    "            'full_distribution': predictions[:top_k]\n",
    "        }\n",
    "\n",
    "# --- Usage Example ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize predictor\n",
    "    predictor = TaxonomyPredictor()\n",
    "    \n",
    "    # Test prediction\n",
    "    texts = [\n",
    "        \"someone needs to look after my grandma\",\n",
    "        \"medicare coverage for seniors\",\n",
    "        \"FDA drug approval process\"\n",
    "    ]\n",
    "    \n",
    "    for text in texts:\n",
    "        result = predictor.predict_taxonomy(text)\n",
    "        print(f\"\\nText: '{text}'\")\n",
    "        print(f\"→ Combined: {result['combined_category']}\")\n",
    "        print(f\"→ L1: {result['l1_category']} (P={result['l1_prob']:.2f})\")\n",
    "        print(f\"→ L2: {result['l2_category']} (P={result['l2_prob']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1764bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your cleaned data (CSV with 'text' column)\n",
    "df = pd.read_csv(\"/Users/gwin/Documents/Post Undergrad Work/Tax Search/test_data/junk/gov_docs.csv\")  \n",
    "\n",
    "# Use tiny-but-mighty model (CPU-friendly)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # 384-dim vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad9d0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing text\n",
    "df = df.dropna(subset=['text']) \n",
    "\n",
    "# Or fill empty values with empty string\n",
    "df['text'] = df['text'].fillna('')\n",
    "df[\"vector\"] = df[\"text\"].apply(lambda x: model.encode(x).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a2d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jh/fclqnxd94096mzl27qjjw9dc0000gn/T/ipykernel_83551/2149986584.py:10: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es.indices.create(index=\"docs\",mappings={\n"
     ]
    }
   ],
   "source": [
    "# from elasticsearch import Elasticsearch\n",
    "\n",
    "# # Start local Elasticsearch (Docker)\n",
    "# # docker run -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:8.12.0\n",
    "\n",
    "# es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "\n",
    "\n",
    "# es.indices.create(index=\"docs\",mappings={\n",
    "#         \"properties\": {\n",
    "#             \"text\": {\"type\": \"text\"},\n",
    "#             \"vector\": {\"type\": \"dense_vector\", \"dims\": 384, \"similarity\": \"cosine\"},\n",
    "#             \"l1_category\": {\"type\": \"keyword\"},  \n",
    "#             \"l2_category\": {\"type\": \"keyword\"},\n",
    "#             \"l1_prob\": {\"type\": \"float\"},\n",
    "#             \"l2_prob\": {\"type\": \"float\"}\n",
    "#         }\n",
    "#     },\n",
    "#     ignore=400  # Ignore \"index already exists\" errors\n",
    "# )\n",
    "# predictor = TaxonomyPredictor() \n",
    "# # Index documents with vectors + taxonomy\n",
    "# for _, row in df.iterrows():\n",
    "#     taxonomy = predictor.predict_taxonomy(row[\"text\"])\n",
    "    \n",
    "#     es.index(\n",
    "#         index=\"docs\",\n",
    "#         document={\n",
    "#             \"text\": row[\"text\"],\n",
    "#             \"vector\": row[\"vector\"],\n",
    "#             \"l1_category\": taxonomy[\"l1_category\"],\n",
    "#             \"l2_category\": taxonomy[\"l2_category\"],\n",
    "#             \"l1_prob\": taxonomy[\"l1_prob\"],\n",
    "#             \"l2_prob\": taxonomy[\"l2_prob\"],\n",
    "#             # Optional: Store full distribution for debugging\n",
    "#             \"metadata\": {\n",
    "#                 \"taxonomy_distribution\": taxonomy[\"full_distribution\"]\n",
    "#             }\n",
    "#         }\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "73f3add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a tiny test dataframe with 4 sample documents\n",
    "test_df = pd.DataFrame([\n",
    "    {\"text\": \"Medicare benefits for elderly patients\", \"id\": \"doc1\"},\n",
    "    {\"text\": \"FDA approves new diabetes medication\", \"id\": \"doc2\"},\n",
    "    {\"text\": \"Budget proposal for defense spending\", \"id\": \"doc3\"},\n",
    "    {\"text\": \"Student loan forgiveness program updates\", \"id\": \"doc4\"}\n",
    "])\n",
    "\n",
    "# Generate vectors (if needed)\n",
    "test_df[\"vector\"] = test_df[\"text\"].apply(lambda x: model.encode(x).tolist())\n",
    "df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "766e0b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 4 documents\n",
      "Indexing complete!\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Initialize Elasticsearch client\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# Delete old index if exists\n",
    "es.indices.delete(index=\"docs\", ignore_unavailable=True)\n",
    "\n",
    "# Create index with triple-category storage\n",
    "es.indices.create(\n",
    "    index=\"docs\",\n",
    "    mappings={\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            # Triple storage for maximum flexibility\n",
    "            \"category\": {\"type\": \"keyword\"},      # \"healthcare|medicare\" (original format)\n",
    "            \"l1\": {\"type\": \"keyword\"},           # \"healthcare\" (extracted)\n",
    "            \"l2\": {\"type\": \"keyword\"},           # \"medicare\" (extracted)\n",
    "            # Confidence metrics\n",
    "            \"confidence\": {\"type\": \"float\"},\n",
    "            \"l1_confidence\": {\"type\": \"float\"},\n",
    "            \"l2_confidence\": {\"type\": \"float\"},\n",
    "            # Full distribution for debugging\n",
    "            \"metadata\": {\"type\": \"object\", \"enabled\": False}\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = TaxonomyPredictor()\n",
    "\n",
    "def index_documents(df):\n",
    "    for _, row in df.iterrows():\n",
    "        taxonomy = predictor.predict_taxonomy(row[\"text\"])\n",
    "        \n",
    "        # Auto-split combined category\n",
    "        l1, l2 = taxonomy[\"combined_category\"].split(\"|\", 1)\n",
    "        \n",
    "        es.index(\n",
    "            index=\"docs\",\n",
    "            document={\n",
    "                \"text\": row[\"text\"],\n",
    "                \"vector\": row[\"vector\"],\n",
    "                # Triple category storage\n",
    "                \"category\": taxonomy[\"combined_category\"],\n",
    "                \"l1\": l1,\n",
    "                \"l2\": l2,\n",
    "                # Confidence metrics\n",
    "                \"confidence\": taxonomy[\"l1_prob\"],\n",
    "                \"l1_confidence\": taxonomy[\"l1_prob\"],\n",
    "                \"l2_confidence\": taxonomy[\"l2_prob\"],\n",
    "                # Debug info\n",
    "                \"metadata\": {\n",
    "                    \"full_distribution\": taxonomy[\"full_distribution\"],\n",
    "                    \"original_prediction\": taxonomy\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    print(f\"Indexed {len(df)} documents\")\n",
    "\n",
    "# Usage\n",
    "index_documents(df)\n",
    "\n",
    "print(\"Indexing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f51e539c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'test_docs'})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dedicated test index\n",
    "test_index = \"test_docs\"\n",
    "es.indices.delete(index=test_index, ignore_unavailable=True)\n",
    "\n",
    "es.indices.create(\n",
    "    index=test_index,\n",
    "    mappings={\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"vector\": {\"type\": \"dense_vector\", \"dims\": 384},\n",
    "            \"category\": {\"type\": \"keyword\"},\n",
    "            \"l1\": {\"type\": \"keyword\"},\n",
    "            \"l2\": {\"type\": \"keyword\"},\n",
    "            \"confidence\": {\"type\": \"float\"}\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f6f9866d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed Test Documents:\n"
     ]
    }
   ],
   "source": [
    "# Check all test documents\n",
    "test_docs = es.search(index=test_index, query={\"match_all\": {}}, size=10)\n",
    "print(\"Indexed Test Documents:\")\n",
    "for hit in test_docs[\"hits\"][\"hits\"]:\n",
    "    doc = hit[\"_source\"]\n",
    "    print(f\"\\nID: {hit['_id']}\")\n",
    "    print(f\"Text: {doc['text']}\")\n",
    "    print(f\"Category: {doc['category']}\")\n",
    "    print(f\"L1: {doc['l1']} | L2: {doc['l2']}\")\n",
    "    print(f\"Confidence: {doc['confidence']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faea78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in test_df.iterrows():\n",
    "    taxonomy = predictor.predict_taxonomy(row[\"text\"])\n",
    "    l1, l2 = taxonomy[\"combined_category\"].split(\"|\", 1)\n",
    "    \n",
    "    es.index(\n",
    "        index=test_index,\n",
    "        id=row[\"id\"],  # Use our test IDs for easy reference\n",
    "        document={\n",
    "            \"text\": row[\"text\"],\n",
    "            \"vector\": row[\"vector\"],\n",
    "            \"category\": taxonomy[\"combined_category\"],\n",
    "            \"l1\": l1,\n",
    "            \"l2\": l2,\n",
    "            \"confidence\": taxonomy[\"l1_prob\"]\n",
    "        }\n",
    "    )\n",
    "print(\"Test documents indexed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "52bf2ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current mapping for l1_category:\n",
      "{'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}\n",
      "\n",
      "Current mapping for l2_category:\n",
      "{'type': 'text', 'fields': {'keyword': {'type': 'keyword', 'ignore_above': 256}}}\n"
     ]
    }
   ],
   "source": [
    "# Get exact mapping for your fields\n",
    "mapping = es.indices.get_mapping(index=\"docs\")\n",
    "print(\"Current mapping for l1_category:\")\n",
    "print(mapping['docs']['mappings']['properties']['l1_category'])\n",
    "print(\"\\nCurrent mapping for l2_category:\")\n",
    "print(mapping['docs']['mappings']['properties']['l2_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "516c66ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing L1 Categories and Counts:\n",
      "defense|procurement: 108 documents\n",
      "education|k12funding: 90 documents\n",
      "healthcare|medicare: 90 documents\n",
      "healthcare|medicaid: 81 documents\n",
      "defense|cybersecurity: 75 documents\n",
      "education|studentloans: 72 documents\n",
      "healthcare|fda: 62 documents\n",
      "education|highereducationpolicy: 60 documents\n",
      "finance|budgets: 54 documents\n",
      "finance|tax_policies: 54 documents\n",
      "healthcare|FDA: 31 documents\n"
     ]
    }
   ],
   "source": [
    "# Get all unique L1 categories (using the correct .keyword field)\n",
    "resp = es.search(\n",
    "    index=\"docs\",\n",
    "    aggs={\n",
    "        \"l1_categories\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"l1_category.keyword\",\n",
    "                \"size\": 100  # Increase if you have many categories\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    size=0\n",
    ")\n",
    "\n",
    "print(\"Existing L1 Categories and Counts:\")\n",
    "for bucket in resp['aggregations']['l1_categories']['buckets']:\n",
    "    print(f\"{bucket['key']}: {bucket['doc_count']} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b866d80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents with 'Health' in L1 category: 0\n",
      "Documents with 'Medical' in L1 category: 0\n",
      "Documents with 'Care' in L1 category: 0\n",
      "Documents with 'Elderly' in L1 category: 0\n",
      "Documents with 'Senior' in L1 category: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for similar categories\n",
    "similar_categories = [\"Health\", \"Medical\", \"Care\", \"Elderly\", \"Senior\"]\n",
    "for cat in similar_categories:\n",
    "    count = es.count(\n",
    "        index=\"docs\",\n",
    "        query={\"wildcard\": {\"l1_category.keyword\": f\"*{cat}*\"}}\n",
    "    )['count']\n",
    "    print(f\"Documents with '{cat}' in L1 category: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d345b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Potentially misclassified documents:\n",
      "\n",
      "ID: FNRpO5YBmBfTKM6rC3Nj\n",
      "Current L1: UNCATEGORIZED\n",
      "Text: Senate Budget and Fiscal Review Committee The Legislature’s Version of the 2020-21 State Budget Summary Unprecedented for this stage of the budget process, the Assembly and Senate are in agreement on ...\n",
      "\n",
      "ID: ktRpO5YBmBfTKM6rB3I6\n",
      "Current L1: UNCATEGORIZED\n",
      "Text: 1 401 Hathaway Building • Cheyenne, WY 82002 Phone (307) 777-7656 • 1-866-571-0944 Fax (307) 777-7439 • www.health.wyo.gov Stefan Johansson Mark Gordon Director Governor February 5, 2025 Dear Medicaid...\n",
      "\n",
      "ID: ndRpO5YBmBfTKM6rB3KZ\n",
      "Current L1: UNCATEGORIZED\n",
      "Text: Module: 11 Medicare Advantage Plans and Other Medicare Plans Inside front cover Module 11: Medicare Advantage Plans and Other Medicare Plans Contents Introduction ........................................\n",
      "\n",
      "ID: V9RpO5YBmBfTKM6rBHKs\n",
      "Current L1: UNCATEGORIZED\n",
      "Text: FDADrug Safety Communication FDA cautions against use of hydroxychloroquine or chloroquine for COVID-19 outside of the hospital setting or a clinical trial due to risk of heart rhythm problems Does no...\n",
      "\n",
      "ID: btRpO5YBmBfTKM6rBnIr\n",
      "Current L1: UNCATEGORIZED\n",
      "Text: ELIJAHWREHGreater Cleveland, OH • +1 (507) 254-3011 • elijahwreh@gmail.com • linkedin.com/in/elijah-wreh-5501031b/ QUALIFICATIONSSUMMARYGlobal Regulation & Compliance • Healthcare • Education • Leader...\n"
     ]
    }
   ],
   "source": [
    "# Find documents that mention healthcare terms but aren't categorized\n",
    "healthcare_terms = [\"healthcare\", \"hospital\", \"elderly\", \"grandma\", \"senior\"]\n",
    "query = {\n",
    "    \"bool\": {\n",
    "        \"must_not\": {\"exists\": {\"field\": \"l1_category.keyword\"}},  # Or use specific bad category\n",
    "        \"should\": [{\"match\": {\"text\": term}} for term in healthcare_terms],\n",
    "        \"minimum_should_match\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "potential_health_docs = es.search(\n",
    "    index=\"docs\",\n",
    "    query=query,\n",
    "    size=5\n",
    ")\n",
    "\n",
    "print(\"\\nPotentially misclassified documents:\")\n",
    "for hit in potential_health_docs['hits']['hits']:\n",
    "    print(f\"\\nID: {hit['_id']}\")\n",
    "    print(f\"Current L1: {hit['_source'].get('l1_category', 'UNCATEGORIZED')}\")\n",
    "    print(f\"Text: {hit['_source']['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9f41e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all Medicare-related documents under Healthcare\n",
    "results = es.search(\n",
    "    index=\"docs\",\n",
    "    query={\n",
    "        \"term\": {\"combined_category\": \"healthcare|medicare\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ccf62aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Initialize models and ES client\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "predictor = TaxonomyPredictor()  # Your taxonomy classifier\n",
    "\n",
    "def hybrid_search(query, l1_filter=None, l2_filter=None, top_k=10):\n",
    "    query_vector = model.encode(query).tolist()\n",
    "    \n",
    "    # Build category filter\n",
    "    category_filter = []\n",
    "    if l1_filter and l2_filter:\n",
    "        category_filter.append({\"term\": {\"combined_category\": f\"{l1_filter}|{l2_filter}\"}})\n",
    "    elif l1_filter:\n",
    "        category_filter.append({\"term\": {\"l1_category\": l1_filter}})\n",
    "    \n",
    "    query_body = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": category_filter,\n",
    "                \"filter\": [{\n",
    "                    \"script_score\": {\n",
    "                        \"query\": {\"match_all\": {}},\n",
    "                        \"script\": {\n",
    "                            \"source\": \"cosineSimilarity(params.query_vector, 'vector') + 1.0\",\n",
    "                            \"params\": {\"query_vector\": query_vector}\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k\n",
    "    }\n",
    "    \n",
    "    return es.search(index=\"docs\", body=query_body)\n",
    "\n",
    "# Example usage:\n",
    "results = hybrid_search(\n",
    "    \"elderly care benefits\",\n",
    "    l1_filter=\"healthcare\",\n",
    "    l2_filter=\"medicare\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a84df1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 Healthcare documents (any subcategory)\n",
      "Found 0 Medicare documents\n",
      "Found 0 documents with L1=healthcare\n",
      "\n",
      "Found 0 results:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Count healthcare documents - THREE OPTIONS:\n",
    "\n",
    "# Option A: Count using combined field (recommended)\n",
    "healthcare_count = es.count(\n",
    "    index=\"docs\",\n",
    "    query={\"prefix\": {\"combined_category.keyword\": \"healthcare|\"}}\n",
    ")['count']\n",
    "print(f\"Found {healthcare_count} Healthcare documents (any subcategory)\")\n",
    "\n",
    "# Option B: Count specific subcategory\n",
    "medicare_count = es.count(\n",
    "    index=\"docs\",\n",
    "    query={\"term\": {\"combined_category.keyword\": \"healthcare|medicare\"}}\n",
    ")['count']\n",
    "print(f\"Found {medicare_count} Medicare documents\")\n",
    "\n",
    "# Option C: Count using separate L1 field\n",
    "healthcare_l1_count = es.count(\n",
    "    index=\"docs\",\n",
    "    query={\"term\": {\"l1_category.keyword\": \"healthcare\"}}\n",
    ")['count']\n",
    "print(f\"Found {healthcare_l1_count} documents with L1=healthcare\")\n",
    "\n",
    "# 2. Updated hybrid search usage\n",
    "results = hybrid_search(\n",
    "    \"elderly care benefits\",\n",
    "    l1_filter=\"healthcare|medicare\",  # Now works with either format\n",
    "    l2_filter=\"medicare\",    # Optional subcategory filter\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# 3. Process results with enhanced output\n",
    "print(f\"\\nFound {len(results['hits']['hits'])} results:\")\n",
    "for i, hit in enumerate(results[\"hits\"][\"hits\"], 1):\n",
    "    source = hit['_source']\n",
    "    print(f\"\\nResult #{i}:\")\n",
    "    print(f\"Score: {hit['_score']:.3f}\")\n",
    "    print(f\"Combined: {source.get('combined_category', 'N/A')}\")\n",
    "    print(f\"L1: {source['l1_category']} (P={source['l1_prob']:.2f})\")\n",
    "    print(f\"L2: {source['l2_category']} (P={source['l2_prob']:.2f})\")\n",
    "    print(f\"Text: {source['text'][:200]}...\")\n",
    "    \n",
    "    # Show full distribution if available\n",
    "    if 'metadata' in source and 'taxonomy_distribution' in source['metadata']:\n",
    "        print(\"\\nTop predictions:\")\n",
    "        for pred in source['metadata']['taxonomy_distribution']:\n",
    "            print(f\"- {pred['label']}: {pred['score']:.2f}\")\n",
    "# results = hybrid_search(\n",
    "#     \"elderly care services\",\n",
    "#     l1_filter=\"Healthcare\",\n",
    "#     top_k=5\n",
    "# )\n",
    "\n",
    "# # 3. Print results\n",
    "# print(f\"\\nFound {len(results['hits']['hits'])} results:\")\n",
    "# for hit in results['hits']['hits']:\n",
    "#     print(f\"\\nScore: {hit['_score']:.3f}\")\n",
    "#     print(f\"L1: {hit['_source']['l1_category']}\")\n",
    "#     print(f\"Text: {hit['_source']['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "786a0dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in index: 1036\n",
      "Sample Healthcare doc: None found\n"
     ]
    }
   ],
   "source": [
    "# Check total documents in index\n",
    "count = es.count(index=\"docs\")['count']\n",
    "print(f\"Total documents in index: {count}\")\n",
    "\n",
    "# Check if any documents have your expected categories\n",
    "sample = es.search(\n",
    "    index=\"docs\",\n",
    "    query={\"term\": {\"l1_category\": \"Healthcare\"}},\n",
    "    size=1\n",
    ")\n",
    "print(\"Sample Healthcare doc:\", sample['hits']['hits'][0]['_source']['text'][:100] if sample['hits']['hits'] else \"None found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
