{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a71f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class TaxonomyPredictor:\n",
    "    def __init__(self, model_path=\"models/model_20250411_122546\"):\n",
    "        \"\"\"Initialize model, tokenizer, and label mappings.\"\"\"\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        # Load label mappings\n",
    "        with open(f\"{model_path}/label_mappings.json\", \"r\") as f:\n",
    "            mappings = json.load(f)\n",
    "            self.id2label = {int(k): v for k, v in mappings[\"id2label\"].items()}\n",
    "            self.label_hierarchy = mappings.get(\"hierarchy\", {})  # Optional: If you have parent-child relationships\n",
    "\n",
    "    def predict_taxonomy(self, text, top_k=3):\n",
    "        \"\"\"Predict L1/L2 categories with probabilities.\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'l1_category': str,       # Main category (e.g., \"healthcare\")\n",
    "                'l2_category': str,       # Subcategory (e.g., \"medicare\")\n",
    "                'combined_category': str, # Combined format \"healthcare|medicare\"\n",
    "                'l1_prob': float,\n",
    "                'l2_prob': float,\n",
    "                'full_distribution': list  # All predictions (sorted)\n",
    "            }\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=1).numpy()[0]\n",
    "        \n",
    "        # Get all predictions\n",
    "        predictions = [\n",
    "            {\"label\": self.id2label[label_id], \"score\": float(prob)}\n",
    "            for label_id, prob in enumerate(probs)\n",
    "        ]\n",
    "        predictions.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        \n",
    "        # Extract L1/L2 from the predicted label\n",
    "        top_pred = predictions[0]['label']\n",
    "        # if ' > ' in top_pred:\n",
    "        #     l1, l2 = top_pred.split(' > ', 1)\n",
    "        #     combined = f\"{l1.lower()}|{l2.lower()}\"  # Format as \"healthcare|medicare\"\n",
    "        # else:\n",
    "        #     l1 = l2 = top_pred\n",
    "        #     combined = f\"{l1.lower()}|{l2.lower()}\"  # Fallback format\n",
    "\n",
    "        if '|' in top_pred:\n",
    "            l1, l2 = top_pred.split('|', 1)  # Split on existing pipe\n",
    "        elif ' > ' in top_pred:\n",
    "            l1, l2 = top_pred.split(' > ', 1)  # Original case\n",
    "        else:\n",
    "            l1 = l2 = top_pred\n",
    "        \n",
    "        # Clean whitespace and standardize case\n",
    "        l1, l2 = l1.strip().lower(), l2.strip().lower()\n",
    "        \n",
    "        return {\n",
    "            'l1_category': l1.lower(),\n",
    "            'l2_category': l2.lower(),\n",
    "            'combined_category': f\"{l1}|{l2}\",  # New field with pipeline format\n",
    "            'l1_prob': predictions[0]['score'],\n",
    "            'l2_prob': predictions[0]['score'],\n",
    "            'full_distribution': predictions[:top_k]\n",
    "        }\n",
    "\n",
    "# # --- Usage Example ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize predictor\n",
    "#     predictor = TaxonomyPredictor()\n",
    "    \n",
    "#     # Test prediction\n",
    "#     texts = [\n",
    "#         \"someone needs to look after my grandma\",\n",
    "#         \"medicare coverage for seniors\",\n",
    "#         \"FDA drug approval process\"\n",
    "#     ]\n",
    "    \n",
    "#     for text in texts:\n",
    "#         result = predictor.predict_taxonomy(text)\n",
    "#         print(f\"\\nText: '{text}'\")\n",
    "#         print(f\"→ Combined: {result['combined_category']}\")\n",
    "#         print(f\"→ L1: {result['l1_category']} (P={result['l1_prob']:.2f})\")\n",
    "#         print(f\"→ L2: {result['l2_category']} (P={result['l2_prob']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a37afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5387c49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mappings inspection:\n",
      "Sample labels: ['healthcare|FDA', 'healthcare|medicaid', 'healthcare|medicare', 'education|highereducationpolicy', 'education|k12funding']\n"
     ]
    }
   ],
   "source": [
    "print(\"Label mappings inspection:\")\n",
    "with open(\"models/model_20250411_122546/label_mappings.json\", \"r\") as f:\n",
    "    mappings = json.load(f)\n",
    "    print(\"Sample labels:\", list(mappings[\"id2label\"].values())[:5])  # Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "364ac7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw predictions:\n",
      "healthcare|FDA: 0.3493\n",
      "healthcare|medicaid: 0.0889\n",
      "healthcare|medicare: 0.1224\n",
      "education|highereducationpolicy: 0.0548\n",
      "education|k12funding: 0.0553\n",
      "education|studentloans: 0.0573\n",
      "finance|tax_policies: 0.0599\n",
      "finance|budgets: 0.0729\n",
      "defense|cybersecurity: 0.0736\n",
      "defense|procurement: 0.0655\n"
     ]
    }
   ],
   "source": [
    "predictor = TaxonomyPredictor()\n",
    "test_text = \"FDA drug approval process\"  # Example that showed duplication\n",
    "inputs = predictor.tokenizer(test_text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = predictor.model(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=1).numpy()[0]\n",
    "\n",
    "print(\"\\nRaw predictions:\")\n",
    "for label_id, prob in enumerate(probs):\n",
    "    print(f\"{predictor.id2label[label_id]}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6450bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "healthcare|fda\n"
     ]
    }
   ],
   "source": [
    "# After implementing the fix:\n",
    "test_text = \"FDA drug approval process\"\n",
    "result = predictor.predict_taxonomy(test_text)\n",
    "print(result['combined_category'])  # Now correctly shows \"healthcare|fda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49f59484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your cleaned data (CSV with 'text' column)\n",
    "df = pd.read_csv(\"/Users/gwin/Documents/Post Undergrad Work/Tax Search/test_data/junk/gov_docs.csv\")  \n",
    "\n",
    "# Use tiny-but-mighty model (CPU-friendly)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # 384-dim vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3732a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing text\n",
    "df = df.dropna(subset=['text']) \n",
    "\n",
    "# Or fill empty values with empty string\n",
    "df['text'] = df['text'].fillna('')\n",
    "df[\"vector\"] = df[\"text\"].apply(lambda x: model.encode(x).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "876d0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "import time\n",
    "\n",
    "class ElasticsearchTaxonomyIndexer:\n",
    "    \n",
    "    def __init__(self, es_host=\"http://localhost:9200\", index_name=\"taxonomy_documents\"):\n",
    "        self.es = Elasticsearch(es_host)\n",
    "        self.index_name = index_name\n",
    "        self.predictor = TaxonomyPredictor()\n",
    "        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "    def create_index(self):\n",
    "        \"\"\"Create Elasticsearch index with proper mappings for taxonomy and vectors\"\"\"\n",
    "        mapping = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"text\": {\"type\": \"text\"},\n",
    "                    \"vector\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": 384,  # Match your SentenceTransformer model\n",
    "                        \"index\": True,\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    },\n",
    "                    \"taxonomy\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"fields\": {\n",
    "                            \"analyzed\": {\"type\": \"text\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"l1_category\": {\"type\": \"keyword\"},\n",
    "                    \"l2_category\": {\"type\": \"keyword\"},\n",
    "                    \"l1_prob\": {\"type\": \"float\"},\n",
    "                    \"l2_prob\": {\"type\": \"float\"},\n",
    "                    \"metadata\": {  # For any additional fields from your CSV\n",
    "                        \"type\": \"object\",\n",
    "                        \"enabled\": True\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Delete index if it exists\n",
    "        if self.es.indices.exists(index=self.index_name):\n",
    "            self.es.indices.delete(index=self.index_name)\n",
    "        \n",
    "        self.es.indices.create(index=self.index_name, body=mapping)\n",
    "    \n",
    "    def process_document(self, row):\n",
    "        \"\"\"Process a single document row from DataFrame\"\"\"\n",
    "        doc = {\n",
    "            \"text\": row[\"text\"],\n",
    "            \"vector\": self.embedding_model.encode(row[\"text\"]).tolist(),\n",
    "            \"metadata\": {k: v for k, v in row.items() if k != \"text\"}\n",
    "        }\n",
    "        \n",
    "        # Add taxonomy classification\n",
    "        taxonomy_result = self.predictor.predict_taxonomy(row[\"text\"])\n",
    "        doc.update({\n",
    "            \"taxonomy\": taxonomy_result[\"combined_category\"],\n",
    "            \"l1_category\": taxonomy_result[\"l1_category\"],\n",
    "            \"l2_category\": taxonomy_result[\"l2_category\"],\n",
    "            \"l1_prob\": taxonomy_result[\"l1_prob\"],\n",
    "            \"l2_prob\": taxonomy_result[\"l2_prob\"]\n",
    "        })\n",
    "        \n",
    "        return doc\n",
    "    \n",
    "    def index_dataframe(self, df, batch_size=100):\n",
    "        \"\"\"Index a pandas DataFrame with taxonomy classification and vectors\"\"\"\n",
    "        actions = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                doc = self.process_document(row)\n",
    "                actions.append({\n",
    "                    \"_index\": self.index_name,\n",
    "                    \"_source\": doc\n",
    "                })\n",
    "                \n",
    "                # Bulk insert in batches\n",
    "                if len(actions) >= batch_size:\n",
    "                    bulk(self.es, actions)\n",
    "                    actions = []\n",
    "                    print(f\"Indexed {batch_size} documents...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing document: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Insert remaining documents\n",
    "        if actions:\n",
    "            bulk(self.es, actions)\n",
    "            print(f\"Indexed final {len(actions)} documents\")\n",
    "        \n",
    "        print(f\"Finished indexing {len(df)} documents\")\n",
    "        self.es.indices.refresh(index=self.index_name)\n",
    "    \n",
    "    def search(self, query, taxonomy_filter=None, top_k=10):\n",
    "        \"\"\"Search with optional taxonomy filtering\"\"\"\n",
    "        # Generate query vector\n",
    "        query_vector = self.embedding_model.encode(query).tolist()\n",
    "        \n",
    "        # Build the search query\n",
    "        search_body = {\n",
    "            \"query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"script\": {\n",
    "                        \"source\": \"cosineSimilarity(params.query_vector, 'vector') + 1.0\",\n",
    "                        \"params\": {\"query_vector\": query_vector}\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"size\": top_k,\n",
    "            \"_source\": [\"text\", \"taxonomy\", \"l1_category\", \"l2_category\"]\n",
    "        }\n",
    "        \n",
    "        # Add taxonomy filter if provided\n",
    "        if taxonomy_filter:\n",
    "            if \"|\" in taxonomy_filter:\n",
    "                l1, l2 = taxonomy_filter.split(\"|\", 1)\n",
    "                search_body[\"query\"][\"script_score\"][\"query\"] = {\n",
    "                    \"bool\": {\n",
    "                        \"must\": [\n",
    "                            {\"term\": {\"l1_category\": l1.lower()}},\n",
    "                            {\"term\": {\"l2_category\": l2.lower()}}\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                search_body[\"query\"][\"script_score\"][\"query\"] = {\n",
    "                    \"term\": {\"l1_category\": taxonomy_filter.lower()}\n",
    "                }\n",
    "        \n",
    "        results = self.es.search(index=self.index_name, body=search_body)\n",
    "        return [hit[\"_source\"] for hit in results[\"hits\"][\"hits\"]]\n",
    "    def smart_search(self, query, use_query_taxonomy=True, top_k=10):\n",
    "        \"\"\"Search using query's predicted taxonomy as filter\"\"\"\n",
    "        # Classify the query itself\n",
    "        query_taxonomy = self.predictor.predict_taxonomy(query)['combined_category']\n",
    "\n",
    "        print(f\"Query classified as: {query_taxonomy}\")  # Debug output\n",
    "\n",
    "        if use_query_taxonomy:\n",
    "            return self.search(query, taxonomy_filter=query_taxonomy, top_k=top_k)\n",
    "        else:\n",
    "            return self.search(query, top_k=top_k)\n",
    "        \n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load your data\n",
    "#     df = pd.read_csv(\"/Users/gwin/Documents/Post Undergrad Work/Tax Search/test_data/junk/gov_docs.csv\")\n",
    "#     df = df.dropna(subset=['text'])\n",
    "#     df['text'] = df['text'].fillna('')\n",
    "    \n",
    "#     # LIMIT TO 4 ENTRIES FOR TESTING (choose one method)\n",
    "#     df = df.head(4)  # First 4 rows (deterministic)\n",
    "#     # OR \n",
    "#     # df = df.sample(4, random_state=42)  # Random 4 rows (reproducible)\n",
    "    \n",
    "#     print(f\"TESTING WITH {len(df)} DOCUMENTS:\")\n",
    "#     print(df['text'].head())  # Verify the subset\n",
    "    \n",
    "#     # Initialize and create index\n",
    "#     indexer = ElasticsearchTaxonomyIndexer()\n",
    "#     indexer.create_index()\n",
    "    \n",
    "#     # Index only the 4 test documents\n",
    "#     indexer.index_dataframe(df)\n",
    "    \n",
    "#     # Example searches (unchanged)\n",
    "#     print(\"\\nSearch results for 'elderly care':\")\n",
    "#     for result in indexer.search(\"i need schooling\"):\n",
    "#         print(f\"{result['taxonomy']}: {result['text'][:100]}...\")\n",
    "    \n",
    "#     print(\"\\nSearch results for 'healthcare' filtered by taxonomy:\")\n",
    "#     for result in indexer.search(\"healthcare\", taxonomy_filter=\"healthcare|medicare\"):\n",
    "#         print(f\"{result['taxonomy']}: {result['text'][:100]}...\")\n",
    "# --- Usage Example ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96f1a98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 100 documents...\n",
      "Indexed 100 documents...\n",
      "Indexed final 59 documents\n",
      "Finished indexing 259 documents\n",
      "\n",
      "Search results for 'elderly care':\n",
      "healthcare|medicare: MAKINGMEDICAREFORBENEFICIARIESBETTERNEWMEDICAREADVANTAGESUPPLEMENTALBENEFITSBeneficiaries in Medicar...\n",
      "healthcare|medicaid: DEPARTMENTOFHEALTH & HUMANSERVICESCenters for Medicare & Medicaid Services 7500 Security Boulevard B...\n",
      "healthcare|medicaid: Department of Health and Hu December 1, 2024 The Honorable Jim Pillen Governor of Nebraska P.O. Box ...\n",
      "healthcare|medicaid: Department of Health and Hu December 1, 2024 The Honorable Jim Pillen Governor of Nebraska P.O. Box ...\n",
      "healthcare|medicaid: Medicaid_10_11_Cvr.indd 1 6/27/2012 11:03:34 AM...\n",
      "healthcare|medicaid: DEPARTMENTOFHEALTH & HUMANSERVICESCenters for Medicare & Medicaid Services 7500 Security Boulevard, ...\n",
      "healthcare|medicare: 11 CHAPTEROptions for slowing the growth of Medicare fee-for-service spending for emergency departme...\n",
      "healthcare|medicaid: Chapter 10: ADescription of Medicaid Eligibility by Paul Gurny, David K. Baugh, and Feather Ann Davi...\n",
      "healthcare|medicare: as March 5, 2024 HP-2024-05 PRIMARYCARESPENDINGINMEDICAREFEE-FOR-SERVICE: ANILLUSTRATIVEANALYSISUSIN...\n",
      "healthcare|medicaid: Medicaid Eligibility Gene Coffey Division of Medicaid Eligibility Policy Medicaid Eligibility Policy...\n",
      "\n",
      "Search results for 'healthcare' filtered by taxonomy:\n",
      "healthcare|medicare: 3 C h a p t e r Medicare’s fee-for-service benefit design 3 Ch a p t e r Medicare’s fee-for-service ...\n",
      "healthcare|medicare: U.S. DEPARTMENTOFHEALTH & HUMANSERVICES 2022 Medicare Fee-for- Service Supplemental Improper Payment...\n",
      "healthcare|medicare: MAKINGMEDICAREFORBENEFICIARIESBETTERNEWMEDICAREADVANTAGESUPPLEMENTALBENEFITSBeneficiaries in Medicar...\n",
      "healthcare|medicare: 11 CHAPTEROptions for slowing the growth of Medicare fee-for-service spending for emergency departme...\n",
      "healthcare|medicare: Centers for Medicare & Medicaid Services National Training Program Frequently Asked Questions (FAQ):...\n",
      "healthcare|medicare: Payment Integrity Scorecard Program or Activity Reporting Period Medicare Fee For Service Q2 2021 Ch...\n",
      "healthcare|medicare: FY2020 Medicare and Medicaid PIReport to Congress (RTC) i Annual Report to Congress – Medicare and M...\n",
      "healthcare|medicare: 2021 REPORTTOCONGRESSMEDICARE & MEDICAIDPROGRAMINTEGRITYU.S. Department of Health and Human Services...\n",
      "healthcare|medicare: Module: 11 Medicare Advantage Plans and Other Medicare Plans Inside front cover Module 11: Medicare ...\n",
      "healthcare|medicare: as March 5, 2024 HP-2024-05 PRIMARYCARESPENDINGINMEDICAREFEE-FOR-SERVICE: ANILLUSTRATIVEANALYSISUSIN...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    df = pd.read_csv(\"/Users/gwin/Documents/Post Undergrad Work/Tax Search/test_data/junk/gov_docs.csv\")\n",
    "    df = df.dropna(subset=['text'])\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    \n",
    "    \n",
    "\n",
    "    # Initialize and create index\n",
    "    indexer = ElasticsearchTaxonomyIndexer()\n",
    "    indexer.create_index()\n",
    "    \n",
    "    # Index your data (this may take some time)\n",
    "    indexer.index_dataframe(df)\n",
    "    \n",
    "    # Example searches\n",
    "    print(\"\\nSearch results for 'elderly care':\")\n",
    "    for result in indexer.search(\"elderly care\"):\n",
    "        print(f\"{result['taxonomy']}: {result['text'][:100]}...\")\n",
    "    \n",
    "    print(\"\\nSearch results for 'healthcare' filtered by taxonomy:\")\n",
    "    for result in indexer.search(\"money\", taxonomy_filter=\"healthcare|medicare\"):\n",
    "        print(f\"{result['taxonomy']}: {result['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b14f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search results for 'elderly care':\n",
      "finance|budgets: 1 March 20, 2024 Fellow Americans, Our nation’s fiscal house is in ruin, the President of the United...\n",
      "finance|budgets: Enacted State Budget Fiscal Year 2023-24 Areas of Interest in the Intersection of Criminal Justice &...\n",
      "education|k12funding: R00A03 Funding for Educational Organizations Maryland State Department of Education Executive Summar...\n",
      "finance|budgets: Senate Budget and Fiscal Review Committee The Legislature’s Version of the 2020-21 State Budget Summ...\n",
      "education|highereducationpolicy: Closing the College Affordability Gap December 2023 A report for The Institute for College Access an...\n",
      "education|k12funding: STATEFACTSHEET: House Republican Proposals Hurt Children, Borrowers, and Undermine Education in New ...\n",
      "education|k12funding: OCTOBER 2023 INVESTMENTSINSTUDENTRECOVERYA Review of School Districts’ Use of American Rescue Plan F...\n",
      "education|highereducationpolicy: Glossary CFRDCLCost of Attendance 2 CHAPTER (Budget) Awards for each of the Federal Student Aid (FSA...\n",
      "finance|budgets: Budget of the U.S. Government FISCALYEAR 2025 OFFICEOFMANAGEMENTANDBUDGETTHEBUDGETDOCUMENTSBudget of...\n",
      "healthcare|medicare: 3 C h a p t e r Medicare’s fee-for-service benefit design 3 Ch a p t e r Medicare’s fee-for-service ...\n",
      "\n",
      "Search results for 'healthcare' filtered by taxonomy:\n",
      "healthcare|medicare: 3 C h a p t e r Medicare’s fee-for-service benefit design 3 Ch a p t e r Medicare’s fee-for-service ...\n",
      "healthcare|medicare: U.S. DEPARTMENTOFHEALTH & HUMANSERVICES 2022 Medicare Fee-for- Service Supplemental Improper Payment...\n",
      "healthcare|medicare: MAKINGMEDICAREFORBENEFICIARIESBETTERNEWMEDICAREADVANTAGESUPPLEMENTALBENEFITSBeneficiaries in Medicar...\n",
      "healthcare|medicare: 11 CHAPTEROptions for slowing the growth of Medicare fee-for-service spending for emergency departme...\n",
      "healthcare|medicare: Centers for Medicare & Medicaid Services National Training Program Frequently Asked Questions (FAQ):...\n",
      "healthcare|medicare: Payment Integrity Scorecard Program or Activity Reporting Period Medicare Fee For Service Q2 2021 Ch...\n",
      "healthcare|medicare: FY2020 Medicare and Medicaid PIReport to Congress (RTC) i Annual Report to Congress – Medicare and M...\n",
      "healthcare|medicare: 2021 REPORTTOCONGRESSMEDICARE & MEDICAIDPROGRAMINTEGRITYU.S. Department of Health and Human Services...\n",
      "healthcare|medicare: Module: 11 Medicare Advantage Plans and Other Medicare Plans Inside front cover Module 11: Medicare ...\n",
      "healthcare|medicare: as March 5, 2024 HP-2024-05 PRIMARYCARESPENDINGINMEDICAREFEE-FOR-SERVICE: ANILLUSTRATIVEANALYSISUSIN...\n"
     ]
    }
   ],
   "source": [
    " print(\"\\nSearch results for 'elderly care':\")\n",
    "for result in indexer.search(\"money\"):\n",
    "    print(f\"{result['taxonomy']}: {result['text'][:100]}...\")\n",
    "\n",
    "print(\"\\nSearch results for 'healthcare' filtered by taxonomy:\")\n",
    "for result in indexer.search(\"money\", taxonomy_filter=\"healthcare|medicare\"):\n",
    "    print(f\"{result['taxonomy']}: {result['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cd56f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: Taxonomy values in index:\n",
      "Taxonomy: healthcare|fda | Text: Product Name Placement, Size, and Prominence in Pr...\n",
      "Taxonomy: education|k12funding | Text: 2024-2025 FISCALYEARANALYSISOFTHENEWJERSEYBUDGETDD...\n",
      "Taxonomy: education|k12funding | Text: 2024-2025 FISCALYEARANALYSISOFTHENEWJERSEYBUDGETDD...\n",
      "Taxonomy: defense|cybersecurity | Text: UNCLASSIFIED//FOROFFICIALUSEONLYPre-Decisional Dra...\n"
     ]
    }
   ],
   "source": [
    "# Add this right after indexing to inspect what was stored\n",
    "print(\"\\nDEBUG: Taxonomy values in index:\")\n",
    "\n",
    "# Use the CORRECT parameter name (top_k instead of size)\n",
    "for doc in indexer.search(\"\", top_k=4):  # Now using 'top_k' which matches your method's parameter\n",
    "    print(f\"Taxonomy: {doc['taxonomy']} | Text: {doc['text'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a1363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query classified as: healthcare|medicare\n"
     ]
    }
   ],
   "source": [
    "results = indexer.smart_search(\"medicare benefits\")\n",
    "for result in results:\n",
    "    print(f\"{result['taxonomy']}: {result['text'][:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
