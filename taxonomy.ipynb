{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scraping https://www.cms.gov/priorities/innovation/evaluation-research-reports...\n",
      "‚úÖ Saved: data/raw/healthcare/Marketplace help desk  call centers.pdf\n",
      "üéâ Downloaded 1 PDFs to data/raw/healthcare\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urljoin\n",
    "\n",
    "# # Configuration\n",
    "# BASE_URL = \"https://www.cms.gov/priorities/innovation/evaluation-research-reports\"  # Replace with your target URL\n",
    "# SAVE_DIR = \"data/raw/healthcare\"\n",
    "# os.makedirs(SAVE_DIR, exist_ok=True)  # Create directory if missing\n",
    "\n",
    "# # Custom headers to mimic browser behavior\n",
    "# HEADERS = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "# }\n",
    "\n",
    "# def sanitize_filename(filename):\n",
    "#     \"\"\"Remove invalid characters from filenames\"\"\"\n",
    "#     return \"\".join(c for c in filename if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "\n",
    "# def download_pdf(url, save_path):\n",
    "#     \"\"\"Download PDF with error handling\"\"\"\n",
    "#     try:\n",
    "#         response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "#         response.raise_for_status()  # Raise HTTP errors\n",
    "#         with open(save_path, 'wb') as f:\n",
    "#             f.write(response.content)\n",
    "#         print(f\"‚úÖ Saved: {save_path}\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Failed to download {url}: {str(e)}\")\n",
    "#         return False\n",
    "\n",
    "# # Main scraping function\n",
    "# def scrape_and_download():\n",
    "#     print(f\"üîç Scraping {BASE_URL}...\")\n",
    "#     try:\n",
    "#         response = requests.get(BASE_URL, headers=HEADERS)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "#         pdf_count = 0\n",
    "#         for link in soup.find_all('a', href=True):\n",
    "#             href = link['href']\n",
    "            \n",
    "#             # Skip non-PDF links\n",
    "#             if not href.lower().endswith('.pdf'):\n",
    "#                 continue\n",
    "                \n",
    "#             # Construct absolute URL\n",
    "#             pdf_url = urljoin(BASE_URL, href)\n",
    "            \n",
    "#             # Generate filename from link text or URL\n",
    "#             filename = sanitize_filename(link.text.strip() or pdf_url.split('/')[-1]) + \".pdf\"\n",
    "#             save_path = os.path.join(SAVE_DIR, filename)\n",
    "            \n",
    "#             # Download PDF\n",
    "#             if download_pdf(pdf_url, save_path):\n",
    "#                 pdf_count += 1\n",
    "        \n",
    "#         print(f\"üéâ Downloaded {pdf_count} PDFs to {SAVE_DIR}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è Scraping failed: {str(e)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_and_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping healthcare from https://www.google.com/search?client=firefox-b-1-d&q=site%3Ahhs.gov+filetype%3Apdf+%22Medicaid%22...\n",
      "No PDFs found for healthcare at https://www.google.com/search?client=firefox-b-1-d&q=site%3Ahhs.gov+filetype%3Apdf+%22Medicaid%22\n",
      "Scraping finance from https://www.treasury.gov/services/Pages/default.aspx...\n",
      "No PDFs found for finance at https://www.treasury.gov/services/Pages/default.aspx\n",
      "Scraping education from https://www.ed.gov/about/reports/annual/index.html...\n",
      "Downloaded: data/raw/education/FY2024-Annual-Agency-Performance-Report.pdf\n",
      "Downloaded: data/raw/education/strategic-plan.pdf\n",
      "Downloaded: data/raw/education/fy24-afr-108470.pdf\n",
      "Downloaded: data/raw/education/agency-financial-report.pdf\n",
      "Downloaded: data/raw/education/agency-financial-report.pdf\n",
      "Downloaded: data/raw/education/agency-financial-report.pdf\n",
      "Downloaded: data/raw/education/agency-financial-report.pdf\n",
      "Downloaded: data/raw/education/agency-financial-report.pdf\n",
      "Downloaded: data/raw/education/agency-financial-report.pdf\n",
      "Downloaded: data/raw/education/agency-financial-report.pdf\n",
      "Downloaded: data/raw/education/agency-financial-report.pdf\n",
      "Downloaded: data/raw/education/FY2024-Annual-Agency-Performance-Report.pdf\n",
      "Downloaded: data/raw/education/fy2023apr-fy2025app.pdf\n",
      "Downloaded: data/raw/education/fy2023apr-fy2025app.pdf\n",
      "Downloaded: data/raw/education/fy2022apr-fy2024app.pdf\n",
      "Downloaded: data/raw/education/fy2022apr-fy2024app.pdf\n",
      "Downloaded: data/raw/education/fy2023app-fy2021apr.pdf\n",
      "Downloaded: data/raw/education/fy2023app-fy2021apr.pdf\n",
      "Downloaded: data/raw/education/fy2022-apr.pdf\n",
      "Downloaded: data/raw/education/fy2022-apr.pdf\n",
      "Downloaded: data/raw/education/fy2020-apr.pdf\n",
      "Downloaded: data/raw/education/fy2020-apr.pdf\n",
      "Downloaded: data/raw/education/fy2019apr-fy2021app-report.pdf\n",
      "Downloaded: data/raw/education/fy2019apr-fy2021app-report.pdf\n",
      "Downloaded: data/raw/education/fy18apr-fy20app.pdf\n",
      "Downloaded: data/raw/education/fy17apr-fy19app.pdf\n",
      "Downloaded: data/raw/education/2016-2018-apr-app.pdf\n",
      "Downloaded: data/raw/education/2015-2017-apr-app.pdf\n",
      "Downloaded: data/raw/education/2014-2016-apr-app-plan.pdf\n",
      "Failed to download https://www.ed.gov/sites/ed/files/about/reports/annual/2015plan/2013-2015-apr-app-plan.pdf: 404 Client Error: Not Found for url: https://www.ed.gov/sites/ed/files/about/reports/annual/2015plan/2013-2015-apr-app-plan.pdf\n",
      "Downloaded: data/raw/education/2024-fsa-annual-report-108481.pdf\n",
      "Downloaded: data/raw/education/fsa-report.pdf\n",
      "Downloaded: data/raw/education/fsa-report.pdf\n",
      "Downloaded: data/raw/education/fsa-report.pdf\n",
      "Downloaded: data/raw/education/fsa-report.pdf\n",
      "Downloaded: data/raw/education/fsa-report.pdf\n",
      "Downloaded: data/raw/education/fsa-report.pdf\n",
      "Downloaded: data/raw/education/fsa-report.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def create_folder_structure(base_dir, categories):\n",
    "    \"\"\"Creates the folder structure for storing documents.\"\"\"\n",
    "    for category in categories:\n",
    "        folder_path = os.path.join(base_dir, 'raw', category)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "def download_pdf(url, save_path):\n",
    "    \"\"\"Downloads a PDF file from a given URL and saves it to the specified path.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded: {save_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "def scrape_pdfs(base_url, category, save_dir):\n",
    "    \"\"\"Scrapes PDF links from a government website and downloads them.\"\"\"\n",
    "    response = requests.get(base_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    pdf_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if href.endswith('.pdf'):\n",
    "            pdf_links.append(urljoin(base_url, href))\n",
    "    \n",
    "    if not pdf_links:\n",
    "        print(f\"No PDFs found for {category} at {base_url}\")\n",
    "        return\n",
    "    \n",
    "    save_path = os.path.join(save_dir, 'raw', category)\n",
    "    for pdf_url in pdf_links:\n",
    "        filename = pdf_url.split('/')[-1]\n",
    "        download_pdf(pdf_url, os.path.join(save_path, filename))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BASE_DIR = \"data\"\n",
    "    CATEGORIES = {\n",
    "        \"healthcare\": \"https://www.cms.gov/priorities/innovation/evaluation-research-reports\",\n",
    "        \"finance\": \"https://www.treasury.gov/services/Pages/default.aspx\",\n",
    "        \"education\": \"https://www.ed.gov/about/reports/annual/index.html\"\n",
    "    }\n",
    "    \n",
    "    create_folder_structure(BASE_DIR, CATEGORIES.keys())\n",
    "    \n",
    "    for category, url in CATEGORIES.items():\n",
    "        print(f\"Scraping {category} from {url}...\")\n",
    "        scrape_pdfs(url, category, BASE_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
