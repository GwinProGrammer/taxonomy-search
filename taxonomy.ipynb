{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scraping https://www.cms.gov/priorities/innovation/evaluation-research-reports...\n",
      "‚úÖ Saved: data/raw/healthcare/Marketplace help desk  call centers.pdf\n",
      "üéâ Downloaded 1 PDFs to data/raw/healthcare\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urljoin\n",
    "\n",
    "# # Configuration\n",
    "# BASE_URL = \"https://www.cms.gov/priorities/innovation/evaluation-research-reports\"  # Replace with your target URL\n",
    "# SAVE_DIR = \"data/raw/healthcare\"\n",
    "# os.makedirs(SAVE_DIR, exist_ok=True)  # Create directory if missing\n",
    "\n",
    "# # Custom headers to mimic browser behavior\n",
    "# HEADERS = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "# }\n",
    "\n",
    "# def sanitize_filename(filename):\n",
    "#     \"\"\"Remove invalid characters from filenames\"\"\"\n",
    "#     return \"\".join(c for c in filename if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "\n",
    "# def download_pdf(url, save_path):\n",
    "#     \"\"\"Download PDF with error handling\"\"\"\n",
    "#     try:\n",
    "#         response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "#         response.raise_for_status()  # Raise HTTP errors\n",
    "#         with open(save_path, 'wb') as f:\n",
    "#             f.write(response.content)\n",
    "#         print(f\"‚úÖ Saved: {save_path}\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Failed to download {url}: {str(e)}\")\n",
    "#         return False\n",
    "\n",
    "# # Main scraping function\n",
    "# def scrape_and_download():\n",
    "#     print(f\"üîç Scraping {BASE_URL}...\")\n",
    "#     try:\n",
    "#         response = requests.get(BASE_URL, headers=HEADERS)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "#         pdf_count = 0\n",
    "#         for link in soup.find_all('a', href=True):\n",
    "#             href = link['href']\n",
    "            \n",
    "#             # Skip non-PDF links\n",
    "#             if not href.lower().endswith('.pdf'):\n",
    "#                 continue\n",
    "                \n",
    "#             # Construct absolute URL\n",
    "#             pdf_url = urljoin(BASE_URL, href)\n",
    "            \n",
    "#             # Generate filename from link text or URL\n",
    "#             filename = sanitize_filename(link.text.strip() or pdf_url.split('/')[-1]) + \".pdf\"\n",
    "#             save_path = os.path.join(SAVE_DIR, filename)\n",
    "            \n",
    "#             # Download PDF\n",
    "#             if download_pdf(pdf_url, save_path):\n",
    "#                 pdf_count += 1\n",
    "        \n",
    "#         print(f\"üéâ Downloaded {pdf_count} PDFs to {SAVE_DIR}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è Scraping failed: {str(e)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_and_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Government Document Collector ===\n",
      "üìÇ Root directory: /Users/gwin/Documents/Post Undergrad Work/Tax Search/data/raw\n",
      "\n",
      "üìÅ Created: data/raw/healthcare\n",
      "üìÅ Created: data/raw/defense\n",
      "\n",
      "üîç Processing HEALTHCARE documents:\n",
      "   - Scanning: https://www.cms.gov/about-cms/agency-information/history\n",
      "‚úÖ Saved: data/raw/healthcare/Admin Tenure Bio 508 July 2015.pdf\n",
      "‚úÖ Saved: data/raw/healthcare/BushSignMMA2003.pdf\n",
      "‚úÖ Saved: data/raw/healthcare/CMS35thAnniversary.pdf\n",
      "‚úÖ Saved: data/raw/healthcare/CMSInBaltimore.pdf\n",
      "‚úÖ Saved: data/raw/healthcare/CMSPresidentsSpeeches.pdf\n",
      "‚úÖ Saved: data/raw/healthcare/MedicareMedicaidMilestones2015_508.pdf\n",
      "‚úÖ Saved: data/raw/healthcare/PresidentCMSMilestones.pdf\n",
      "‚úÖ Saved: data/raw/healthcare/QUIZ08.pdf\n",
      "‚úÖ Saved: data/raw/healthcare/Agent-Broker-Help-Desks Updates_051024_0.pdf\n",
      "   - Scanning: https://www.medicaid.gov/about-us/reports-and-evaluations/index.html\n",
      "‚ö†Ô∏è Scraping error at https://www.medicaid.gov/about-us/reports-and-evaluations/index.html: 404 Client Error: Not Found for url: https://www.medicaid.gov/about-us/reports-and-evaluations/index.html\n",
      "   ‚ö†Ô∏è No PDFs found using standard methods\n",
      "\n",
      "üîç Processing DEFENSE documents:\n",
      "   - Scanning: https://www.defense.gov/News/Publications/\n",
      "‚úÖ Saved: data/raw/defense/DOD-STRATEGIC-MGMT-PLAN-2023.PDF\n",
      "‚úÖ Saved: data/raw/defense/defense_1744139405.pdf\n",
      "‚úÖ Saved: data/raw/defense/_skip-target\n",
      "‚úÖ Saved: data/raw/defense/defense_1744139409.pdf\n",
      "‚úÖ Saved: data/raw/defense/defense_1744139412.pdf\n",
      "‚úÖ Saved: data/raw/defense/defense_1744139414.pdf\n",
      "‚úÖ Saved: data/raw/defense/defense_1744139416.pdf\n",
      "   - Scanning: https://media.defense.gov/Publications/\n",
      "‚ö†Ô∏è Scraping error at https://media.defense.gov/Publications/: 404 Client Error: Not Found for url: https://media.defense.gov/Publications/\n",
      "   ‚ö†Ô∏è No PDFs found using standard methods\n",
      "\n",
      "üèÅ Collection complete! Files saved to project's data/raw folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Configuration - works in notebooks/IDEs/standalone scripts\n",
    "try:\n",
    "    BASE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\", \"raw\")\n",
    "except NameError:\n",
    "    BASE_DIR = os.path.join(os.getcwd(), \"data\", \"raw\")  # Fallback for notebooks\n",
    "CATEGORIES = {\n",
    "    \"healthcare\": [\n",
    "        \"https://www.cms.gov/about-cms/agency-information/history\",\n",
    "        \"https://www.medicaid.gov/about-us/reports-and-evaluations/index.html\"\n",
    "    ],\n",
    "    \"defense\": [\n",
    "        \"https://www.defense.gov/News/Publications/\",\n",
    "        \"https://media.defense.gov/Publications/\"\n",
    "    ]\n",
    "}\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "}\n",
    "DELAY = 2  # Conservative delay for .gov sites\n",
    "\n",
    "def setup_project_folders():\n",
    "    \"\"\"Create ./data/raw/[category] structure in current project\"\"\"\n",
    "    for category in CATEGORIES:\n",
    "        category_path = os.path.join(BASE_DIR, category)\n",
    "        os.makedirs(category_path, exist_ok=True)\n",
    "        print(f\"üìÅ Created: {os.path.relpath(category_path)}\")\n",
    "\n",
    "def is_gov_pdf(url):\n",
    "    \"\"\"Enhanced PDF detection for government sites\"\"\"\n",
    "    if not url:\n",
    "        return False\n",
    "    url_lower = url.lower()\n",
    "    return (\n",
    "        url_lower.endswith('.pdf') or\n",
    "        '/pdf' in url_lower or\n",
    "        any(x in url_lower for x in [\n",
    "            'download=pdf',\n",
    "            'type=pdf',\n",
    "            '/document_library/',\n",
    "            '/publications/',\n",
    "            'file=pdf'\n",
    "        ])\n",
    "    )\n",
    "\n",
    "def download_gov_pdf(url, save_path):\n",
    "    \"\"\"Government-specific downloader with robust handling\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url,\n",
    "            headers=HEADERS,\n",
    "            stream=True,\n",
    "            verify=False,\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Handle Content-Disposition filenames\n",
    "        if 'Content-Disposition' in response.headers:\n",
    "            filename = re.findall(\n",
    "                'filename=\"?([^\"]+)\"?',\n",
    "                response.headers['Content-Disposition']\n",
    "            )[0]\n",
    "            save_path = os.path.join(os.path.dirname(save_path), filename)\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        print(f\"‚úÖ Saved: {os.path.relpath(save_path)}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to download {os.path.basename(url)}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def scrape_gov_site(base_url):\n",
    "    \"\"\"Specialized scraper for government document portals\"\"\"\n",
    "    try:\n",
    "        time.sleep(DELAY)\n",
    "        response = requests.get(base_url, headers=HEADERS, verify=False, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        pdf_links = set()  # Avoid duplicates\n",
    "\n",
    "        # Government sites often use these patterns\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = urljoin(base_url, link['href'])\n",
    "            if is_gov_pdf(href):\n",
    "                pdf_links.add(href)\n",
    "\n",
    "        return sorted(pdf_links)[:10]  # Return first 10 unique PDFs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Scraping error at {base_url}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    print(\"\\n=== Government Document Collector ===\")\n",
    "    print(f\"üìÇ Root directory: {os.path.abspath(BASE_DIR)}\\n\")\n",
    "    setup_project_folders()\n",
    "\n",
    "    for category, urls in CATEGORIES.items():\n",
    "        print(f\"\\nüîç Processing {category.upper()} documents:\")\n",
    "        \n",
    "        for url in urls:\n",
    "            print(f\"   - Scanning: {url}\")\n",
    "            pdfs = scrape_gov_site(url)\n",
    "            \n",
    "            if not pdfs:\n",
    "                print(\"   ‚ö†Ô∏è No PDFs found using standard methods\")\n",
    "                continue\n",
    "                \n",
    "            category_dir = os.path.join(BASE_DIR, category)\n",
    "            for pdf_url in pdfs:\n",
    "                # Generate clean filename\n",
    "                filename = (\n",
    "                    os.path.basename(pdf_url.split('?')[0])\n",
    "                    or f\"{category}_{int(time.time())}.pdf\"\n",
    "                )\n",
    "                # Replace spaces and special chars\n",
    "                filename = re.sub(r'[^\\w\\-.]', '_', filename)\n",
    "                \n",
    "                save_path = os.path.join(category_dir, filename)\n",
    "                download_gov_pdf(pdf_url, save_path)\n",
    "                time.sleep(DELAY)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"\\nüèÅ Collection complete! Files saved to project's data/raw folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
