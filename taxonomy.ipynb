{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urljoin\n",
    "\n",
    "# # Configuration\n",
    "# BASE_URL = \"https://www.cms.gov/priorities/innovation/evaluation-research-reports\"  # Replace with your target URL\n",
    "# SAVE_DIR = \"data/raw/healthcare\"\n",
    "# os.makedirs(SAVE_DIR, exist_ok=True)  # Create directory if missing\n",
    "\n",
    "# # Custom headers to mimic browser behavior\n",
    "# HEADERS = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "# }\n",
    "\n",
    "# def sanitize_filename(filename):\n",
    "#     \"\"\"Remove invalid characters from filenames\"\"\"\n",
    "#     return \"\".join(c for c in filename if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "\n",
    "# def download_pdf(url, save_path):\n",
    "#     \"\"\"Download PDF with error handling\"\"\"\n",
    "#     try:\n",
    "#         response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "#         response.raise_for_status()  # Raise HTTP errors\n",
    "#         with open(save_path, 'wb') as f:\n",
    "#             f.write(response.content)\n",
    "#         print(f\"‚úÖ Saved: {save_path}\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Failed to download {url}: {str(e)}\")\n",
    "#         return False\n",
    "\n",
    "# # Main scraping function\n",
    "# def scrape_and_download():\n",
    "#     print(f\"üîç Scraping {BASE_URL}...\")\n",
    "#     try:\n",
    "#         response = requests.get(BASE_URL, headers=HEADERS)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "#         pdf_count = 0\n",
    "#         for link in soup.find_all('a', href=True):\n",
    "#             href = link['href']\n",
    "            \n",
    "#             # Skip non-PDF links\n",
    "#             if not href.lower().endswith('.pdf'):\n",
    "#                 continue\n",
    "                \n",
    "#             # Construct absolute URL\n",
    "#             pdf_url = urljoin(BASE_URL, href)\n",
    "            \n",
    "#             # Generate filename from link text or URL\n",
    "#             filename = sanitize_filename(link.text.strip() or pdf_url.split('/')[-1]) + \".pdf\"\n",
    "#             save_path = os.path.join(SAVE_DIR, filename)\n",
    "            \n",
    "#             # Download PDF\n",
    "#             if download_pdf(pdf_url, save_path):\n",
    "#                 pdf_count += 1\n",
    "        \n",
    "#         print(f\"üéâ Downloaded {pdf_count} PDFs to {SAVE_DIR}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è Scraping failed: {str(e)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     scrape_and_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import time\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from urllib.parse import urljoin\n",
    "# import urllib3\n",
    "# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "\n",
    "# # Configuration - works in notebooks/IDEs/standalone scripts\n",
    "# try:\n",
    "#     BASE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\", \"raw\")\n",
    "# except NameError:\n",
    "#     BASE_DIR = os.path.join(os.getcwd(), \"data\", \"raw\")  # Fallback for notebooks\n",
    "# CATEGORIES = {\n",
    "#     \"healthcare\": [\n",
    "#         \"https://www.cms.gov/about-cms/agency-information/history\",\n",
    "#         \"https://www.medicaid.gov/about-us/reports-and-evaluations/index.html\"\n",
    "#     ],\n",
    "#     \"defense\": [\n",
    "#         \"https://www.defense.gov/News/Publications/\",\n",
    "#         \"https://www.esd.whs.mil/DD/\",\n",
    "#         \"https://media.defense.gov/Publications/\"\n",
    "#     ],\n",
    "#     \"education\": [\n",
    "#         \"https://www2.ed.gov/about/reports/annual/index.html\",\n",
    "#         \"https://nces.ed.gov/pubsearch/\",\n",
    "#         \"https://www2.ed.gov/policy/gen/leg/foia/library.html\"\n",
    "#     ],\n",
    "#     \"finance\": [\n",
    "#         \"https://www.federalreserve.gov/publications.htm\",\n",
    "#         \"https://home.treasury.gov/policy-issues/financial-markets-financial-institutions-and-fiscal-service/fsoc/reports\",\n",
    "#         \"https://www.sec.gov/reports.shtml\"\n",
    "#     ]\n",
    "# }\n",
    "# # HEADERS = {\n",
    "# #     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "# # }\n",
    "# HEADERS = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "#     'Accept': 'application/pdf, text/html'\n",
    "# }\n",
    "# DELAY = 2  # Conservative delay for .gov sites\n",
    "\n",
    "# def setup_project_folders():\n",
    "#     \"\"\"Create ./data/raw/[category] structure in current project\"\"\"\n",
    "#     for category in CATEGORIES:\n",
    "#         category_path = os.path.join(BASE_DIR, category)\n",
    "#         os.makedirs(category_path, exist_ok=True)\n",
    "#         print(f\"üìÅ Created: {os.path.relpath(category_path)}\")\n",
    "\n",
    "# def is_gov_pdf(url):\n",
    "#     \"\"\"Enhanced PDF detection for government sites\"\"\"\n",
    "#     if not url:\n",
    "#         return False\n",
    "#     url_lower = url.lower()\n",
    "#     return (\n",
    "#         url_lower.endswith('.pdf') or\n",
    "#         '/pdf' in url_lower or\n",
    "#         any(x in url_lower for x in [\n",
    "#             'download=pdf',\n",
    "#             'type=pdf',\n",
    "#             '/document_library/',\n",
    "#             '/publications/',\n",
    "#             'file=pdf'\n",
    "#         ])\n",
    "#     )\n",
    "\n",
    "# def download_gov_pdf(url, save_path):\n",
    "#     \"\"\"Government-specific downloader with robust handling\"\"\"\n",
    "#     try:\n",
    "#         response = requests.get(\n",
    "#             url,\n",
    "#             headers=HEADERS,\n",
    "#             stream=True,\n",
    "#             verify=False,\n",
    "#             timeout=30\n",
    "#         )\n",
    "#         response.raise_for_status()\n",
    "\n",
    "#         # Handle Content-Disposition filenames\n",
    "#         if 'Content-Disposition' in response.headers:\n",
    "#             filename = re.findall(\n",
    "#                 'filename=\"?([^\"]+)\"?',\n",
    "#                 response.headers['Content-Disposition']\n",
    "#             )[0]\n",
    "#             save_path = os.path.join(os.path.dirname(save_path), filename)\n",
    "\n",
    "#         with open(save_path, 'wb') as f:\n",
    "#             for chunk in response.iter_content(8192):\n",
    "#                 f.write(chunk)\n",
    "        \n",
    "#         print(f\"‚úÖ Saved: {os.path.relpath(save_path)}\")\n",
    "#         return True\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Failed to download {os.path.basename(url)}: {str(e)}\")\n",
    "#         return False\n",
    "\n",
    "# def scrape_gov_site(base_url):\n",
    "#     \"\"\"Specialized scraper for government document portals\"\"\"\n",
    "#     try:\n",
    "#         time.sleep(DELAY)\n",
    "#         response = requests.get(base_url, headers=HEADERS, verify=False, timeout=30)\n",
    "#         response.raise_for_status()\n",
    "#         # \n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         pdf_links = set()  # Avoid duplicates\n",
    "\n",
    "#         # Government sites often use these patterns\n",
    "#         for link in soup.find_all('a', href=True):\n",
    "#             href = urljoin(base_url, link['href'])\n",
    "#             if is_gov_pdf(href):\n",
    "#                 pdf_links.add(href)\n",
    "\n",
    "#         return sorted(pdf_links)[:10]  # Return first 10 unique PDFs\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ö†Ô∏è Scraping error at {base_url}: {str(e)}\")\n",
    "#         return []\n",
    "\n",
    "# def main():\n",
    "#     print(\"\\n=== Government Document Collector ===\")\n",
    "#     print(f\"üìÇ Root directory: {os.path.abspath(BASE_DIR)}\\n\")\n",
    "#     setup_project_folders()\n",
    "\n",
    "#     for category, urls in CATEGORIES.items():\n",
    "#         print(f\"\\nüîç Processing {category.upper()} documents:\")\n",
    "        \n",
    "#         for url in urls:\n",
    "#             print(f\"   - Scanning: {url}\")\n",
    "#             pdfs = scrape_gov_site(url)\n",
    "            \n",
    "#             if not pdfs:\n",
    "#                 print(\"   ‚ö†Ô∏è No PDFs found using standard methods\")\n",
    "#                 continue\n",
    "                \n",
    "#             category_dir = os.path.join(BASE_DIR, category)\n",
    "#             for pdf_url in pdfs:\n",
    "#                 # Generate clean filename\n",
    "#                 filename = (\n",
    "#                     os.path.basename(pdf_url.split('?')[0])\n",
    "#                     or f\"{category}_{int(time.time())}.pdf\"\n",
    "#                 )\n",
    "#                 # Replace spaces and special chars\n",
    "#                 filename = re.sub(r'[^\\w\\-.]', '_', filename)\n",
    "                \n",
    "#                 save_path = os.path.join(category_dir, filename)\n",
    "#                 download_gov_pdf(pdf_url, save_path)\n",
    "#                 time.sleep(DELAY)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "#     print(\"\\nüèÅ Collection complete! Files saved to project's data/raw folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created download folder at: /Users/gwin/Documents/Post Undergrad Work/Tax Search/data/raw/test\n",
      "\n",
      "üîç Scraping Google results from: PASTE_GOOGLE_SEARCH_URL_HERE...\n",
      "‚ùå Scraping failed: Invalid URL 'PASTE_GOOGLE_SEARCH_URL_HERE': No scheme supplied. Perhaps you meant https://PASTE_GOOGLE_SEARCH_URL_HERE?\n",
      "\n",
      "‚ö†Ô∏è No PDF links found. Possible issues:\n",
      "- Google is blocking the scraper\n",
      "- No PDF results on the search page\n",
      "- Try a different search query\n",
      "\n",
      "üèÅ Check your downloads in: /Users/gwin/Documents/Post Undergrad Work/Tax Search/data/raw/test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, unquote\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# Configuration\n",
    "SEARCH_URL = \"PASTE_GOOGLE_SEARCH_URL_HERE\"  # e.g. \"https://www.google.com/search?q=site:defense.gov+filetype:pdf+report\"\n",
    "DOWNLOAD_FOLDER = \"data/raw/test\"  # Changed to your requested path\n",
    "MAX_PDFS = 5\n",
    "DELAY = 2  # seconds between downloads\n",
    "\n",
    "def setup():\n",
    "    \"\"\"Create download folder with subdirectories\"\"\"\n",
    "    os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)\n",
    "    print(f\"üìÅ Created download folder at: {os.path.abspath(DOWNLOAD_FOLDER)}\")\n",
    "\n",
    "def get_random_headers():\n",
    "    \"\"\"Generate rotating headers to avoid blocking\"\"\"\n",
    "    ua = UserAgent()\n",
    "    return {\n",
    "        'User-Agent': ua.random,\n",
    "        'Accept': 'text/html,application/pdf',\n",
    "        'Referer': 'https://www.google.com/'\n",
    "    }\n",
    "\n",
    "def extract_pdf_links(url):\n",
    "    \"\"\"Improved PDF link extraction\"\"\"\n",
    "    print(f\"\\nüîç Scraping Google results from: {url[:80]}...\")\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url,\n",
    "            headers=get_random_headers(),\n",
    "            timeout=15\n",
    "        )\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        pdf_links = []\n",
    "        for a in soup.select('a[href^=\"/url?q=\"]'):  # Google result links\n",
    "            try:\n",
    "                raw_link = a['href'].split('&')[0]\n",
    "                link = unquote(raw_link.replace('/url?q=', ''))\n",
    "                if link.lower().endswith('.pdf'):\n",
    "                    pdf_links.append(link)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        return list(set(pdf_links))[:MAX_PDFS]  # Remove duplicates and limit\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Scraping failed: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def download_pdf(url):\n",
    "    \"\"\"More reliable PDF downloader\"\"\"\n",
    "    try:\n",
    "        # Generate filename\n",
    "        filename = os.path.basename(urlparse(url).path)\n",
    "        if not filename.lower().endswith('.pdf'):\n",
    "            filename = f\"document_{int(time.time())}.pdf\"\n",
    "        \n",
    "        save_path = os.path.join(DOWNLOAD_FOLDER, filename)\n",
    "        \n",
    "        print(f\"‚¨áÔ∏è Downloading: {filename[:50]}...\")\n",
    "        response = requests.get(\n",
    "            url,\n",
    "            headers=get_random_headers(),\n",
    "            stream=True,\n",
    "            timeout=30,\n",
    "            verify=False  # Bypass SSL verification for .gov/.mil sites\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Stream download with progress\n",
    "        with open(save_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        # Verify download size\n",
    "        file_size = os.path.getsize(save_path) / 1024  # KB\n",
    "        if file_size < 5:  # Skip very small files\n",
    "            os.remove(save_path)\n",
    "            print(f\"‚ùå Removed small file (only {file_size:.1f} KB)\")\n",
    "            return False\n",
    "            \n",
    "        print(f\"‚úÖ Saved {filename} ({file_size:.1f} KB)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    setup()\n",
    "    pdf_links = extract_pdf_links(SEARCH_URL)\n",
    "    \n",
    "    if not pdf_links:\n",
    "        print(\"\\n‚ö†Ô∏è No PDF links found. Possible issues:\")\n",
    "        print(\"- Google is blocking the scraper\")\n",
    "        print(\"- No PDF results on the search page\")\n",
    "        print(\"- Try a different search query\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(pdf_links)} PDFs:\")\n",
    "    for i, url in enumerate(pdf_links, 1):\n",
    "        print(f\"{i}. {url[:80]}...\")\n",
    "    \n",
    "    print(\"\\nStarting downloads...\")\n",
    "    for url in pdf_links:\n",
    "        download_pdf(url)\n",
    "        time.sleep(DELAY)  # Be polite to servers\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(f\"\\nüèÅ Check your downloads in: {os.path.abspath(DOWNLOAD_FOLDER)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import os\n",
    "\n",
    "def pdf_to_text(folder_path):\n",
    "    texts = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            texts[filename] = extract_text(filepath)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "PDFSyntaxError",
     "evalue": "No /Root object! - Is this really a PDF?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPDFSyntaxError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhealthcare\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefense\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meducation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m {category: pdf_to_text(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/raw/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m categories}\n",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhealthcare\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefense\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meducation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m {category: \u001b[43mpdf_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/raw/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcategory\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m categories}\n",
      "Cell \u001b[0;32mIn[35], line 9\u001b[0m, in \u001b[0;36mpdf_to_text\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      8\u001b[0m         filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, filename)\n\u001b[0;32m----> 9\u001b[0m         texts[filename] \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m texts\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/taxo/lib/python3.9/site-packages/pdfminer/high_level.py:177\u001b[0m, in \u001b[0;36mextract_text\u001b[0;34m(pdf_file, password, page_numbers, maxpages, caching, codec, laparams)\u001b[0m\n\u001b[1;32m    174\u001b[0m device \u001b[38;5;241m=\u001b[39m TextConverter(rsrcmgr, output_string, codec\u001b[38;5;241m=\u001b[39mcodec, laparams\u001b[38;5;241m=\u001b[39mlaparams)\n\u001b[1;32m    175\u001b[0m interpreter \u001b[38;5;241m=\u001b[39m PDFPageInterpreter(rsrcmgr, device)\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m PDFPage\u001b[38;5;241m.\u001b[39mget_pages(\n\u001b[1;32m    178\u001b[0m     fp,\n\u001b[1;32m    179\u001b[0m     page_numbers,\n\u001b[1;32m    180\u001b[0m     maxpages\u001b[38;5;241m=\u001b[39mmaxpages,\n\u001b[1;32m    181\u001b[0m     password\u001b[38;5;241m=\u001b[39mpassword,\n\u001b[1;32m    182\u001b[0m     caching\u001b[38;5;241m=\u001b[39mcaching,\n\u001b[1;32m    183\u001b[0m ):\n\u001b[1;32m    184\u001b[0m     interpreter\u001b[38;5;241m.\u001b[39mprocess_page(page)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_string\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/taxo/lib/python3.9/site-packages/pdfminer/pdfpage.py:159\u001b[0m, in \u001b[0;36mPDFPage.get_pages\u001b[0;34m(cls, fp, pagenos, maxpages, password, caching, check_extractable)\u001b[0m\n\u001b[1;32m    157\u001b[0m parser \u001b[38;5;241m=\u001b[39m PDFParser(fp)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Create a PDF document object that stores the document structure.\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mPDFDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Check if the document allows text extraction.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# If not, warn the user and proceed.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mis_extractable:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/taxo/lib/python3.9/site-packages/pdfminer/pdfdocument.py:738\u001b[0m, in \u001b[0;36mPDFDocument.__init__\u001b[0;34m(self, parser, password, caching, fallback)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFSyntaxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo /Root object! - Is this really a PDF?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m LITERAL_CATALOG:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mSTRICT:\n",
      "\u001b[0;31mPDFSyntaxError\u001b[0m: No /Root object! - Is this really a PDF?"
     ]
    }
   ],
   "source": [
    "categories = ['healthcare', 'defense', 'finance', 'education']\n",
    "raw_data = {category: pdf_to_text(f'data/raw/{category}') for category in categories}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
